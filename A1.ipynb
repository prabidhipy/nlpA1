{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad1af6a",
   "metadata": {},
   "source": [
    "# Word Embeddings from Scratch (Skip-gram, Negative Sampling, GloVe)\n",
    "\n",
    "**Name:** Prabidhi Pyakurel  \n",
    "**Task:** Word Embeddings from Scratch + Evaluation + Web Application\n",
    "\n",
    "## Datasets Used\n",
    "- **Reuters-21578 corpus** (via NLTK)  \n",
    "  David D. Lewis, 1997\n",
    "- **Word Analogy Dataset**  \n",
    "  Mikolov et al., 2013\n",
    "- **WordSim-353**  \n",
    "  Finkelstein et al., 2001\n",
    "\n",
    "All embedding models in this notebook are implemented **from scratch** without using any pretrained vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d520868",
   "metadata": {},
   "source": [
    "## TASK 1\n",
    "\n",
    "This task implements and trains three word embedding models:\n",
    "1. Skip-gram with full softmax\n",
    "2. Skip-gram with Negative Sampling\n",
    "3. GloVe\n",
    "\n",
    "The models are trained on the Reuters-21578 corpus after basic text cleaning, tokenization, and vocabulary construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79bba4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3a9354e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/prabidhi/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/prabidhi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#loading reuters dataset\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def load_reuters():\n",
    "    fileids = reuters.fileids()\n",
    "    corpus = []\n",
    "    for fid in fileids:\n",
    "        words = [w.lower() for w in reuters.words(fid)]\n",
    "        words = [re.sub(r\"[^a-z]\", \"\", w) for w in words]\n",
    "        words = [w for w in words if w]\n",
    "        if len(words) > 5:\n",
    "            corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "corpus = load_reuters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b57d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7efde9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Building\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus)))\n",
    "vocabs.append('<UNK>')\n",
    "word2index = {v: idx for idx, v in enumerate(vocabs)}\n",
    "index2word = {idx: v for v, idx in word2index.items()}\n",
    "voc_size = len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff5bc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for sequences\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20e7f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dynamic window size\n",
    "def get_skipgrams_dynamic(corpus, max_window=2):\n",
    "    skipgrams = []\n",
    "    for doc in corpus:\n",
    "        for i in range(len(doc)):\n",
    "            w = random.randint(1, max_window)\n",
    "            for j in range(-w, w + 1):\n",
    "                if j == 0 or i + j < 0 or i + j >= len(doc):\n",
    "                    continue\n",
    "                skipgrams.append([\n",
    "                    word2index.get(doc[i], word2index['<UNK>']),\n",
    "                    word2index.get(doc[i+j], word2index['<UNK>'])\n",
    "                ])\n",
    "    return skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1e48a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, skipgrams):\n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "    return np.array(inputs), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6647c1",
   "metadata": {},
   "source": [
    "### 1. Word2Vec - Without Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74174e",
   "metadata": {},
   "source": [
    "### Skip-gram with Full Softmax\n",
    "\n",
    "The Skip-gram model predicts surrounding context words given a center word.\n",
    "A full softmax is computed over the entire vocabulary, which makes training expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e06e001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center) \n",
    "        outside_embedding = self.embedding_outside(outside)\n",
    "        all_vocabs_embedding = self.embedding_outside(all_vocabs)\n",
    "\n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1).reshape(-1, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cfdf33a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram Without Negative Sampling\n",
      "Epoch 500 | Loss: 22.755888\n",
      "Epoch 1000 | Loss: 19.811848\n",
      "Epoch 1500 | Loss: 18.712788\n",
      "Epoch 2000 | Loss: 19.172894\n"
     ]
    }
   ],
   "source": [
    "# Training Setup\n",
    "emb_size = 50\n",
    "batch_size = 64\n",
    "window_size = 2 # DYNAMIC WINDOW SIZE\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "\n",
    "model_sg = Skipgram(voc_size, emb_size)\n",
    "optimizer_sg = optim.Adam(model_sg.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Skipgram Without Negative Sampling\")\n",
    "skipgrams = get_skipgrams_dynamic(corpus, window_size)\n",
    "\n",
    "start_sg = time.time()\n",
    "for epoch in range(2000):\n",
    "    input_batch, label_batch = random_batch(batch_size, skipgrams)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    loss = model_sg(input_tensor, label_tensor, all_vocabs)\n",
    "\n",
    "    optimizer_sg.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_sg.step()\n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "final_loss_sg = loss.item()\n",
    "time_sg = time.time() - start_sg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e76164",
   "metadata": {},
   "source": [
    "### 2. Word2Vec - Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d3e74",
   "metadata": {},
   "source": [
    "### Skip-gram with Negative Sampling\n",
    "\n",
    "Negative Sampling works by updating only a small number of negative samples per training step.\n",
    "This significantly reduces computation and improves training speed while preserving semantic quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "562cfc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram distribution for negative sampling\n",
    "z = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "809c865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(flatten(corpus))\n",
    "num_total_words = sum(word_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "25265862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 109,\n",
       "         'to': 67,\n",
       "         'of': 67,\n",
       "         'in': 57,\n",
       "         'said': 51,\n",
       "         'and': 51,\n",
       "         'a': 50,\n",
       "         'mln': 40,\n",
       "         's': 35,\n",
       "         'vs': 33,\n",
       "         'for': 32,\n",
       "         'dlrs': 30,\n",
       "         'it': 27,\n",
       "         'pct': 25,\n",
       "         'on': 24,\n",
       "         'from': 22,\n",
       "         'lt': 22,\n",
       "         'cts': 22,\n",
       "         'that': 20,\n",
       "         'is': 20,\n",
       "         'year': 20,\n",
       "         'its': 20,\n",
       "         'net': 19,\n",
       "         'by': 19,\n",
       "         'at': 19,\n",
       "         'u': 18,\n",
       "         'be': 18,\n",
       "         'with': 17,\n",
       "         'was': 17,\n",
       "         'will': 17,\n",
       "         'billion': 17,\n",
       "         'loss': 15,\n",
       "         'he': 15,\n",
       "         'an': 14,\n",
       "         'company': 14,\n",
       "         'has': 14,\n",
       "         'would': 14,\n",
       "         'as': 14,\n",
       "         'not': 13,\n",
       "         'shr': 13,\n",
       "         'inc': 13,\n",
       "         'which': 12,\n",
       "         'but': 11,\n",
       "         'oil': 11,\n",
       "         'corp': 11,\n",
       "         'bank': 11,\n",
       "         'this': 11,\n",
       "         'profit': 10,\n",
       "         'trade': 10,\n",
       "         'last': 10,\n",
       "         'share': 10,\n",
       "         'or': 10,\n",
       "         'were': 10,\n",
       "         'one': 10,\n",
       "         'have': 10,\n",
       "         'had': 10,\n",
       "         'are': 10,\n",
       "         'market': 9,\n",
       "         'shares': 9,\n",
       "         'about': 9,\n",
       "         'they': 9,\n",
       "         'also': 9,\n",
       "         'stock': 9,\n",
       "         'qtr': 9,\n",
       "         'new': 9,\n",
       "         'two': 9,\n",
       "         'tonnes': 9,\n",
       "         'revs': 8,\n",
       "         'may': 8,\n",
       "         'up': 8,\n",
       "         'prices': 8,\n",
       "         'sales': 8,\n",
       "         'per': 7,\n",
       "         'first': 7,\n",
       "         'february': 7,\n",
       "         'april': 7,\n",
       "         'rate': 7,\n",
       "         'co': 7,\n",
       "         'price': 7,\n",
       "         'japan': 7,\n",
       "         'quarter': 7,\n",
       "         'after': 7,\n",
       "         'march': 7,\n",
       "         'been': 7,\n",
       "         'group': 7,\n",
       "         'than': 7,\n",
       "         'more': 7,\n",
       "         'no': 6,\n",
       "         'production': 6,\n",
       "         'agreement': 6,\n",
       "         'note': 6,\n",
       "         'other': 6,\n",
       "         'three': 6,\n",
       "         'government': 6,\n",
       "         'tax': 6,\n",
       "         'over': 6,\n",
       "         'january': 6,\n",
       "         'if': 6,\n",
       "         'offer': 6,\n",
       "         'we': 6,\n",
       "         'exchange': 6,\n",
       "         'week': 6,\n",
       "         'could': 6,\n",
       "         'against': 6,\n",
       "         'dollar': 6,\n",
       "         'total': 5,\n",
       "         'nine': 5,\n",
       "         'stg': 5,\n",
       "         'rise': 5,\n",
       "         'current': 5,\n",
       "         'world': 5,\n",
       "         'exports': 5,\n",
       "         'dlr': 5,\n",
       "         'end': 5,\n",
       "         'five': 5,\n",
       "         'some': 5,\n",
       "         'increase': 5,\n",
       "         'interest': 5,\n",
       "         'under': 5,\n",
       "         'record': 5,\n",
       "         'board': 5,\n",
       "         'today': 5,\n",
       "         'all': 5,\n",
       "         'international': 5,\n",
       "         'oper': 5,\n",
       "         'six': 5,\n",
       "         'growth': 5,\n",
       "         'foreign': 5,\n",
       "         'their': 5,\n",
       "         'expected': 5,\n",
       "         'there': 5,\n",
       "         'rates': 5,\n",
       "         'official': 5,\n",
       "         'told': 5,\n",
       "         'rose': 5,\n",
       "         'month': 5,\n",
       "         'ltd': 5,\n",
       "         'export': 4,\n",
       "         'countries': 4,\n",
       "         'mths': 4,\n",
       "         'wheat': 4,\n",
       "         'due': 4,\n",
       "         'central': 4,\n",
       "         'cash': 4,\n",
       "         'added': 4,\n",
       "         'analysts': 4,\n",
       "         'years': 4,\n",
       "         'december': 4,\n",
       "         'investment': 4,\n",
       "         'ec': 4,\n",
       "         'companies': 4,\n",
       "         'avg': 4,\n",
       "         'major': 4,\n",
       "         'shrs': 4,\n",
       "         'earlier': 4,\n",
       "         'sources': 4,\n",
       "         'business': 4,\n",
       "         'th': 4,\n",
       "         'earnings': 4,\n",
       "         'japanese': 4,\n",
       "         'unit': 4,\n",
       "         'meeting': 4,\n",
       "         'department': 4,\n",
       "         'months': 4,\n",
       "         'stake': 4,\n",
       "         'officials': 4,\n",
       "         'states': 4,\n",
       "         'money': 4,\n",
       "         'west': 4,\n",
       "         'operations': 4,\n",
       "         'president': 4,\n",
       "         'should': 4,\n",
       "         'into': 4,\n",
       "         'between': 4,\n",
       "         'cut': 4,\n",
       "         'further': 4,\n",
       "         'american': 4,\n",
       "         'says': 4,\n",
       "         'yen': 4,\n",
       "         'any': 4,\n",
       "         'compared': 4,\n",
       "         'includes': 4,\n",
       "         'products': 4,\n",
       "         'economic': 4,\n",
       "         'when': 4,\n",
       "         'lower': 4,\n",
       "         'national': 4,\n",
       "         'industry': 4,\n",
       "         'agreed': 4,\n",
       "         'buy': 4,\n",
       "         'common': 4,\n",
       "         'united': 4,\n",
       "         'down': 4,\n",
       "         'financial': 4,\n",
       "         'acquisition': 4,\n",
       "         'imports': 4,\n",
       "         'banks': 4,\n",
       "         'four': 4,\n",
       "         'around': 4,\n",
       "         'spokesman': 4,\n",
       "         'deficit': 4,\n",
       "         'because': 4,\n",
       "         'pay': 4,\n",
       "         'fell': 4,\n",
       "         'sale': 4,\n",
       "         'now': 4,\n",
       "         'while': 4,\n",
       "         'made': 4,\n",
       "         'dividend': 4,\n",
       "         'report': 4,\n",
       "         'south': 3,\n",
       "         'term': 3,\n",
       "         'federal': 3,\n",
       "         'markets': 3,\n",
       "         'acquire': 3,\n",
       "         'most': 3,\n",
       "         'owned': 3,\n",
       "         'grain': 3,\n",
       "         'supply': 3,\n",
       "         'agriculture': 3,\n",
       "         'gas': 3,\n",
       "         'terms': 3,\n",
       "         'output': 3,\n",
       "         'st': 3,\n",
       "         'i': 3,\n",
       "         'average': 3,\n",
       "         'merger': 3,\n",
       "         'make': 3,\n",
       "         'rd': 3,\n",
       "         'split': 3,\n",
       "         'announced': 3,\n",
       "         'firm': 3,\n",
       "         'plans': 3,\n",
       "         'out': 3,\n",
       "         'sold': 3,\n",
       "         'chairman': 3,\n",
       "         'take': 3,\n",
       "         'o': 3,\n",
       "         'p': 3,\n",
       "         'minister': 3,\n",
       "         'industries': 3,\n",
       "         'likely': 3,\n",
       "         'div': 3,\n",
       "         'dealers': 3,\n",
       "         'k': 3,\n",
       "         'through': 3,\n",
       "         'economy': 3,\n",
       "         'ago': 3,\n",
       "         'plan': 3,\n",
       "         'annual': 3,\n",
       "         'crude': 3,\n",
       "         'income': 3,\n",
       "         'based': 3,\n",
       "         'did': 3,\n",
       "         'higher': 3,\n",
       "         'sell': 3,\n",
       "         'industrial': 3,\n",
       "         'next': 3,\n",
       "         'shareholders': 3,\n",
       "         'loan': 3,\n",
       "         'ended': 3,\n",
       "         'capital': 3,\n",
       "         'union': 3,\n",
       "         'june': 3,\n",
       "         'european': 3,\n",
       "         'trading': 3,\n",
       "         'securities': 3,\n",
       "         'management': 3,\n",
       "         'expects': 3,\n",
       "         'assets': 3,\n",
       "         'time': 3,\n",
       "         'talks': 3,\n",
       "         't': 3,\n",
       "         'high': 3,\n",
       "         'fall': 3,\n",
       "         'system': 3,\n",
       "         'state': 3,\n",
       "         'only': 3,\n",
       "         'surplus': 3,\n",
       "         'fed': 3,\n",
       "         'seven': 3,\n",
       "         'before': 3,\n",
       "         'still': 3,\n",
       "         'canadian': 3,\n",
       "         'commission': 3,\n",
       "         'policy': 3,\n",
       "         'same': 3,\n",
       "         'however': 3,\n",
       "         'gold': 3,\n",
       "         'domestic': 3,\n",
       "         'canada': 3,\n",
       "         'coffee': 3,\n",
       "         'currency': 3,\n",
       "         'yesterday': 3,\n",
       "         'tender': 3,\n",
       "         'set': 3,\n",
       "         'recent': 3,\n",
       "         'eight': 3,\n",
       "         'credit': 3,\n",
       "         'traders': 3,\n",
       "         'his': 3,\n",
       "         'prior': 3,\n",
       "         'during': 3,\n",
       "         'subsidiary': 3,\n",
       "         'period': 3,\n",
       "         'nil': 3,\n",
       "         'half': 3,\n",
       "         'since': 3,\n",
       "         'day': 3,\n",
       "         'bid': 3,\n",
       "         'both': 3,\n",
       "         'who': 3,\n",
       "         'reserves': 3,\n",
       "         'statement': 3,\n",
       "         'can': 3,\n",
       "         'gain': 3,\n",
       "         'outstanding': 3,\n",
       "         'level': 3,\n",
       "         'sugar': 3,\n",
       "         'reported': 3,\n",
       "         'such': 3,\n",
       "         'demand': 3,\n",
       "         'general': 3,\n",
       "         'results': 3,\n",
       "         'debt': 3,\n",
       "         'figures': 3,\n",
       "         'long': 3,\n",
       "         'corn': 3,\n",
       "         'until': 2,\n",
       "         'above': 2,\n",
       "         'taiwan': 2,\n",
       "         'held': 2,\n",
       "         'account': 2,\n",
       "         'issue': 2,\n",
       "         'sector': 2,\n",
       "         'profits': 2,\n",
       "         'reagan': 2,\n",
       "         'possible': 2,\n",
       "         'conference': 2,\n",
       "         'each': 2,\n",
       "         'payments': 2,\n",
       "         'early': 2,\n",
       "         'accord': 2,\n",
       "         'help': 2,\n",
       "         'our': 2,\n",
       "         'previous': 2,\n",
       "         'second': 2,\n",
       "         'increased': 2,\n",
       "         'public': 2,\n",
       "         'service': 2,\n",
       "         'decline': 2,\n",
       "         'french': 2,\n",
       "         'october': 2,\n",
       "         'target': 2,\n",
       "         'administration': 2,\n",
       "         'association': 2,\n",
       "         'loans': 2,\n",
       "         'barrel': 2,\n",
       "         'paris': 2,\n",
       "         'firms': 2,\n",
       "         'result': 2,\n",
       "         'reserve': 2,\n",
       "         'fourth': 2,\n",
       "         'full': 2,\n",
       "         'continue': 2,\n",
       "         'plant': 2,\n",
       "         'revenues': 2,\n",
       "         'members': 2,\n",
       "         'off': 2,\n",
       "         'late': 2,\n",
       "         'levels': 2,\n",
       "         'non': 2,\n",
       "         'see': 2,\n",
       "         'show': 2,\n",
       "         'preferred': 2,\n",
       "         'largest': 2,\n",
       "         'say': 2,\n",
       "         'called': 2,\n",
       "         'far': 2,\n",
       "         'soviet': 2,\n",
       "         'washington': 2,\n",
       "         'deal': 2,\n",
       "         'news': 2,\n",
       "         'part': 2,\n",
       "         'systems': 2,\n",
       "         'purchase': 2,\n",
       "         'treasury': 2,\n",
       "         'support': 2,\n",
       "         'york': 2,\n",
       "         'reduce': 2,\n",
       "         'these': 2,\n",
       "         'stocks': 2,\n",
       "         'here': 2,\n",
       "         'l': 2,\n",
       "         'line': 2,\n",
       "         'community': 2,\n",
       "         'savings': 2,\n",
       "         'acquired': 2,\n",
       "         'them': 2,\n",
       "         'open': 2,\n",
       "         'western': 2,\n",
       "         'much': 2,\n",
       "         'effective': 2,\n",
       "         'less': 2,\n",
       "         'base': 2,\n",
       "         'reuters': 2,\n",
       "         'pacific': 2,\n",
       "         'finance': 2,\n",
       "         'proposed': 2,\n",
       "         'low': 2,\n",
       "         'action': 2,\n",
       "         'so': 2,\n",
       "         'london': 2,\n",
       "         'need': 2,\n",
       "         'private': 2,\n",
       "         'petroleum': 2,\n",
       "         'losses': 2,\n",
       "         'following': 2,\n",
       "         'transaction': 2,\n",
       "         'additional': 2,\n",
       "         'prime': 2,\n",
       "         'within': 2,\n",
       "         'china': 2,\n",
       "         'committee': 2,\n",
       "         'futures': 2,\n",
       "         'sees': 2,\n",
       "         'opec': 2,\n",
       "         'noted': 2,\n",
       "         'several': 2,\n",
       "         'product': 2,\n",
       "         'goods': 2,\n",
       "         'executive': 2,\n",
       "         'cents': 2,\n",
       "         'takeover': 2,\n",
       "         'well': 2,\n",
       "         'farm': 2,\n",
       "         'balance': 2,\n",
       "         'cocoa': 2,\n",
       "         'intervention': 2,\n",
       "         'basis': 2,\n",
       "         'including': 2,\n",
       "         'gulf': 2,\n",
       "         'adjusted': 2,\n",
       "         'bundesbank': 2,\n",
       "         'n': 2,\n",
       "         'move': 2,\n",
       "         'short': 2,\n",
       "         'already': 2,\n",
       "         'close': 2,\n",
       "         'plc': 2,\n",
       "         'services': 2,\n",
       "         'crop': 2,\n",
       "         'budget': 2,\n",
       "         'trust': 2,\n",
       "         'nations': 2,\n",
       "         'texas': 2,\n",
       "         'operating': 2,\n",
       "         'revised': 2,\n",
       "         'banking': 2,\n",
       "         'funds': 2,\n",
       "         'drop': 2,\n",
       "         'contract': 2,\n",
       "         'below': 2,\n",
       "         'making': 2,\n",
       "         'jan': 2,\n",
       "         'import': 2,\n",
       "         'include': 2,\n",
       "         'baker': 2,\n",
       "         'must': 2,\n",
       "         'cost': 2,\n",
       "         'm': 2,\n",
       "         'might': 2,\n",
       "         'raised': 2,\n",
       "         'german': 2,\n",
       "         'francs': 2,\n",
       "         'according': 2,\n",
       "         'what': 2,\n",
       "         'comment': 2,\n",
       "         'proposal': 2,\n",
       "         'given': 2,\n",
       "         'resources': 2,\n",
       "         'number': 2,\n",
       "         'germany': 2,\n",
       "         'secretary': 2,\n",
       "         'barrels': 2,\n",
       "         'days': 2,\n",
       "         'program': 2,\n",
       "         'costs': 2,\n",
       "         'council': 2,\n",
       "         'british': 2,\n",
       "         'asked': 2,\n",
       "         'fiscal': 2,\n",
       "         'third': 2,\n",
       "         'declined': 2,\n",
       "         'good': 2,\n",
       "         'value': 2,\n",
       "         'does': 2,\n",
       "         'country': 2,\n",
       "         'development': 2,\n",
       "         'decision': 2,\n",
       "         'usda': 2,\n",
       "         'subject': 2,\n",
       "         'real': 2,\n",
       "         'september': 2,\n",
       "         'approved': 2,\n",
       "         'another': 2,\n",
       "         'inflation': 2,\n",
       "         'brazil': 2,\n",
       "         'ministry': 2,\n",
       "         'being': 2,\n",
       "         'payable': 2,\n",
       "         'analyst': 2,\n",
       "         'very': 2,\n",
       "         'holdings': 2,\n",
       "         'consumer': 2,\n",
       "         'energy': 2,\n",
       "         'future': 2,\n",
       "         'bpd': 2,\n",
       "         'control': 2,\n",
       "         'those': 2,\n",
       "         'july': 2,\n",
       "         'bill': 2,\n",
       "         'change': 2,\n",
       "         'point': 2,\n",
       "         'sets': 2,\n",
       "         'completed': 2,\n",
       "         'use': 2,\n",
       "         'producers': 2,\n",
       "         'previously': 2,\n",
       "         'large': 2,\n",
       "         'house': 2,\n",
       "         'quota': 2,\n",
       "         'approval': 2,\n",
       "         'estimated': 2,\n",
       "         'amount': 2,\n",
       "         'bought': 2,\n",
       "         'pact': 2,\n",
       "         'marks': 2,\n",
       "         'feb': 2,\n",
       "         'do': 2,\n",
       "         'agency': 2,\n",
       "         'strong': 2,\n",
       "         'qtly': 2,\n",
       "         'monetary': 2,\n",
       "         'forecast': 2,\n",
       "         'data': 2,\n",
       "         'many': 2,\n",
       "         'key': 1,\n",
       "         'left': 1,\n",
       "         'moves': 1,\n",
       "         'rights': 1,\n",
       "         'main': 1,\n",
       "         'gains': 1,\n",
       "         'e': 1,\n",
       "         'dixons': 1,\n",
       "         'discuss': 1,\n",
       "         'purchased': 1,\n",
       "         'mine': 1,\n",
       "         'charge': 1,\n",
       "         'malaysia': 1,\n",
       "         'project': 1,\n",
       "         'notes': 1,\n",
       "         'prospects': 1,\n",
       "         'makes': 1,\n",
       "         'provisions': 1,\n",
       "         'african': 1,\n",
       "         'sea': 1,\n",
       "         'bancorp': 1,\n",
       "         'chemical': 1,\n",
       "         'cyclops': 1,\n",
       "         'telephone': 1,\n",
       "         'property': 1,\n",
       "         'certain': 1,\n",
       "         'falls': 1,\n",
       "         'contracts': 1,\n",
       "         'produced': 1,\n",
       "         'completes': 1,\n",
       "         'producing': 1,\n",
       "         'tin': 1,\n",
       "         'view': 1,\n",
       "         'india': 1,\n",
       "         'law': 1,\n",
       "         'areas': 1,\n",
       "         'buys': 1,\n",
       "         'stability': 1,\n",
       "         'quotas': 1,\n",
       "         'africa': 1,\n",
       "         'regular': 1,\n",
       "         'later': 1,\n",
       "         'institute': 1,\n",
       "         'partnership': 1,\n",
       "         'mid': 1,\n",
       "         'war': 1,\n",
       "         'directors': 1,\n",
       "         'aimed': 1,\n",
       "         'showed': 1,\n",
       "         'california': 1,\n",
       "         'repurchase': 1,\n",
       "         'improved': 1,\n",
       "         'remains': 1,\n",
       "         'venture': 1,\n",
       "         'legislation': 1,\n",
       "         'remaining': 1,\n",
       "         'fully': 1,\n",
       "         'package': 1,\n",
       "         'discontinued': 1,\n",
       "         'liquidity': 1,\n",
       "         'farmers': 1,\n",
       "         'palm': 1,\n",
       "         'impact': 1,\n",
       "         'quoted': 1,\n",
       "         'little': 1,\n",
       "         'how': 1,\n",
       "         'november': 1,\n",
       "         'capacity': 1,\n",
       "         'morning': 1,\n",
       "         'review': 1,\n",
       "         'usair': 1,\n",
       "         'borg': 1,\n",
       "         'political': 1,\n",
       "         'communications': 1,\n",
       "         'efforts': 1,\n",
       "         'mines': 1,\n",
       "         'traded': 1,\n",
       "         'normal': 1,\n",
       "         'h': 1,\n",
       "         'commercial': 1,\n",
       "         'receive': 1,\n",
       "         'although': 1,\n",
       "         'policies': 1,\n",
       "         'iran': 1,\n",
       "         'initial': 1,\n",
       "         'light': 1,\n",
       "         'conditions': 1,\n",
       "         'chicago': 1,\n",
       "         'needed': 1,\n",
       "         'daily': 1,\n",
       "         'city': 1,\n",
       "         'definitive': 1,\n",
       "         'ico': 1,\n",
       "         'court': 1,\n",
       "         'unlikely': 1,\n",
       "         'joint': 1,\n",
       "         'lost': 1,\n",
       "         'cattle': 1,\n",
       "         'order': 1,\n",
       "         'ccc': 1,\n",
       "         'form': 1,\n",
       "         'former': 1,\n",
       "         'england': 1,\n",
       "         'index': 1,\n",
       "         'east': 1,\n",
       "         'proposals': 1,\n",
       "         'limit': 1,\n",
       "         'commerce': 1,\n",
       "         'offset': 1,\n",
       "         'regulatory': 1,\n",
       "         'allied': 1,\n",
       "         'difficult': 1,\n",
       "         'available': 1,\n",
       "         'competitive': 1,\n",
       "         'like': 1,\n",
       "         'reason': 1,\n",
       "         'pressure': 1,\n",
       "         'sells': 1,\n",
       "         'equipment': 1,\n",
       "         'back': 1,\n",
       "         'ecuador': 1,\n",
       "         'indonesia': 1,\n",
       "         'buyout': 1,\n",
       "         'purolator': 1,\n",
       "         'equity': 1,\n",
       "         'hit': 1,\n",
       "         'existing': 1,\n",
       "         'source': 1,\n",
       "         'northern': 1,\n",
       "         'keep': 1,\n",
       "         'tokyo': 1,\n",
       "         'gdp': 1,\n",
       "         'currencies': 1,\n",
       "         'concern': 1,\n",
       "         'italy': 1,\n",
       "         'properties': 1,\n",
       "         'guilders': 1,\n",
       "         'g': 1,\n",
       "         'used': 1,\n",
       "         'ending': 1,\n",
       "         'figure': 1,\n",
       "         'dispute': 1,\n",
       "         'increases': 1,\n",
       "         'corporate': 1,\n",
       "         'sterling': 1,\n",
       "         'overseas': 1,\n",
       "         'going': 1,\n",
       "         'points': 1,\n",
       "         'beef': 1,\n",
       "         'respectively': 1,\n",
       "         'transactions': 1,\n",
       "         'letter': 1,\n",
       "         'cover': 1,\n",
       "         'times': 1,\n",
       "         'purchases': 1,\n",
       "         'class': 1,\n",
       "         'holding': 1,\n",
       "         'air': 1,\n",
       "         'bp': 1,\n",
       "         'date': 1,\n",
       "         'filed': 1,\n",
       "         'means': 1,\n",
       "         'adding': 1,\n",
       "         'buffer': 1,\n",
       "         'worth': 1,\n",
       "         'details': 1,\n",
       "         'monday': 1,\n",
       "         'decided': 1,\n",
       "         'food': 1,\n",
       "         'discount': 1,\n",
       "         'call': 1,\n",
       "         'gencorp': 1,\n",
       "         'provision': 1,\n",
       "         'longer': 1,\n",
       "         'barley': 1,\n",
       "         'become': 1,\n",
       "         'volume': 1,\n",
       "         'took': 1,\n",
       "         'near': 1,\n",
       "         'pre': 1,\n",
       "         'predicted': 1,\n",
       "         'head': 1,\n",
       "         'quarterly': 1,\n",
       "         'manager': 1,\n",
       "         'increasing': 1,\n",
       "         'periods': 1,\n",
       "         'tons': 1,\n",
       "         'add': 1,\n",
       "         'put': 1,\n",
       "         'posted': 1,\n",
       "         'where': 1,\n",
       "         'earned': 1,\n",
       "         'swiss': 1,\n",
       "         'grow': 1,\n",
       "         'offered': 1,\n",
       "         'leading': 1,\n",
       "         'great': 1,\n",
       "         'return': 1,\n",
       "         'released': 1,\n",
       "         'agricultural': 1,\n",
       "         'signed': 1,\n",
       "         'warner': 1,\n",
       "         'final': 1,\n",
       "         'diluted': 1,\n",
       "         'gross': 1,\n",
       "         'area': 1,\n",
       "         'planned': 1,\n",
       "         'principle': 1,\n",
       "         'interview': 1,\n",
       "         'j': 1,\n",
       "         'plus': 1,\n",
       "         'field': 1,\n",
       "         'reports': 1,\n",
       "         'member': 1,\n",
       "         'attack': 1,\n",
       "         'pound': 1,\n",
       "         'came': 1,\n",
       "         'start': 1,\n",
       "         'few': 1,\n",
       "         'maize': 1,\n",
       "         'shipment': 1,\n",
       "         'accepted': 1,\n",
       "         'discussions': 1,\n",
       "         'crowns': 1,\n",
       "         'dec': 1,\n",
       "         'buying': 1,\n",
       "         'cable': 1,\n",
       "         'majority': 1,\n",
       "         'singapore': 1,\n",
       "         'oils': 1,\n",
       "         'organization': 1,\n",
       "         'rejected': 1,\n",
       "         'totalled': 1,\n",
       "         'august': 1,\n",
       "         'vice': 1,\n",
       "         'recovery': 1,\n",
       "         'range': 1,\n",
       "         'combined': 1,\n",
       "         'season': 1,\n",
       "         'wants': 1,\n",
       "         'met': 1,\n",
       "         'enough': 1,\n",
       "         'spain': 1,\n",
       "         'place': 1,\n",
       "         'silver': 1,\n",
       "         'gasoline': 1,\n",
       "         'division': 1,\n",
       "         'she': 1,\n",
       "         'pretax': 1,\n",
       "         'situation': 1,\n",
       "         'small': 1,\n",
       "         'ships': 1,\n",
       "         'remain': 1,\n",
       "         'began': 1,\n",
       "         'seen': 1,\n",
       "         'seek': 1,\n",
       "         'begin': 1,\n",
       "         'competition': 1,\n",
       "         'newspaper': 1,\n",
       "         'probably': 1,\n",
       "         'gave': 1,\n",
       "         'marketing': 1,\n",
       "         'tuesday': 1,\n",
       "         'spending': 1,\n",
       "         'hold': 1,\n",
       "         'name': 1,\n",
       "         'statistics': 1,\n",
       "         'mark': 1,\n",
       "         'just': 1,\n",
       "         'france': 1,\n",
       "         'problem': 1,\n",
       "         'local': 1,\n",
       "         'able': 1,\n",
       "         'britain': 1,\n",
       "         'disclosed': 1,\n",
       "         'remained': 1,\n",
       "         'ag': 1,\n",
       "         'won': 1,\n",
       "         'yet': 1,\n",
       "         'director': 1,\n",
       "         'estate': 1,\n",
       "         'oecd': 1,\n",
       "         'dollars': 1,\n",
       "         'shearson': 1,\n",
       "         'started': 1,\n",
       "         'particularly': 1,\n",
       "         'undisclosed': 1,\n",
       "         'bags': 1,\n",
       "         'cuts': 1,\n",
       "         'then': 1,\n",
       "         'act': 1,\n",
       "         'bills': 1,\n",
       "         'research': 1,\n",
       "         'overall': 1,\n",
       "         'activity': 1,\n",
       "         'charges': 1,\n",
       "         'partners': 1,\n",
       "         'commodity': 1,\n",
       "         'think': 1,\n",
       "         'soybeans': 1,\n",
       "         'lyng': 1,\n",
       "         'substantial': 1,\n",
       "         'brazilian': 1,\n",
       "         'offers': 1,\n",
       "         'north': 1,\n",
       "         'supplies': 1,\n",
       "         'working': 1,\n",
       "         'believe': 1,\n",
       "         'shortage': 1,\n",
       "         'related': 1,\n",
       "         'senior': 1,\n",
       "         'required': 1,\n",
       "         'interests': 1,\n",
       "         'continued': 1,\n",
       "         'taking': 1,\n",
       "         'revenue': 1,\n",
       "         'raise': 1,\n",
       "         'dutch': 1,\n",
       "         'closing': 1,\n",
       "         'option': 1,\n",
       "         'steel': 1,\n",
       "         'bond': 1,\n",
       "         'twa': 1,\n",
       "         'lead': 1,\n",
       "         'stores': 1,\n",
       "         'aid': 1,\n",
       "         'hard': 1,\n",
       "         'lending': 1,\n",
       "         'press': 1,\n",
       "         'winter': 1,\n",
       "         'officer': 1,\n",
       "         'turnover': 1,\n",
       "         'almost': 1,\n",
       "         'units': 1,\n",
       "         'involved': 1,\n",
       "         'businesses': 1,\n",
       "         'rather': 1,\n",
       "         'expand': 1,\n",
       "         'similar': 1,\n",
       "         'extended': 1,\n",
       "         'selling': 1,\n",
       "         'come': 1,\n",
       "         'meet': 1,\n",
       "         'excludes': 1,\n",
       "         'c': 1,\n",
       "         'colombia': 1,\n",
       "         'unless': 1,\n",
       "         'shipping': 1,\n",
       "         'maintain': 1,\n",
       "         'ship': 1,\n",
       "         'even': 1,\n",
       "         'office': 1,\n",
       "         'nd': 1,\n",
       "         'paper': 1,\n",
       "         'beginning': 1,\n",
       "         'computer': 1,\n",
       "         'study': 1,\n",
       "         'zealand': 1,\n",
       "         'restructuring': 1,\n",
       "         'issues': 1,\n",
       "         'express': 1,\n",
       "         'least': 1,\n",
       "         'boost': 1,\n",
       "         'nearly': 1,\n",
       "         'own': 1,\n",
       "         'construction': 1,\n",
       "         'congress': 1,\n",
       "         'television': 1,\n",
       "         'factors': 1,\n",
       "         'agreements': 1,\n",
       "         'minimum': 1,\n",
       "         'provided': 1,\n",
       "         'retail': 1,\n",
       "         'addition': 1,\n",
       "         'acquisitions': 1,\n",
       "         'position': 1,\n",
       "         'tomorrow': 1,\n",
       "         'again': 1,\n",
       "         'bonus': 1,\n",
       "         'extraordinary': 1,\n",
       "         'bring': 1,\n",
       "         'rubber': 1,\n",
       "         'security': 1,\n",
       "         'risk': 1,\n",
       "         'payment': 1,\n",
       "         'investors': 1,\n",
       "         'exporters': 1,\n",
       "         'speculation': 1,\n",
       "         'f': 1,\n",
       "         'power': 1,\n",
       "         'soybean': 1,\n",
       "         'filing': 1,\n",
       "         'natural': 1,\n",
       "         'holders': 1,\n",
       "         'white': 1,\n",
       "         'royal': 1,\n",
       "         'fund': 1,\n",
       "         'distribution': 1,\n",
       "         'acres': 1,\n",
       "         'gnp': 1,\n",
       "         'led': 1,\n",
       "         'best': 1,\n",
       "         'yeutter': 1,\n",
       "         'mining': 1,\n",
       "         'bankers': 1,\n",
       "         'recently': 1,\n",
       "         'home': 1,\n",
       "         'unemployment': 1,\n",
       "         'go': 1,\n",
       "         'consumption': 1,\n",
       "         'top': 1,\n",
       "         'scheduled': 1,\n",
       "         'certificates': 1,\n",
       "         'mainly': 1,\n",
       "         'louvre': 1,\n",
       "         'ct': 1,\n",
       "         'allow': 1,\n",
       "         'right': 1,\n",
       "         'semiconductor': 1,\n",
       "         'heavy': 1,\n",
       "         'consolidated': 1,\n",
       "         'latest': 1,\n",
       "         'free': 1,\n",
       "         'ore': 1,\n",
       "         'raises': 1,\n",
       "         'producer': 1,\n",
       "         'included': 1,\n",
       "         'series': 1,\n",
       "         'dropped': 1,\n",
       "         'cause': 1,\n",
       "         'offering': 1,\n",
       "         'operation': 1,\n",
       "         'nation': 1,\n",
       "         'reach': 1,\n",
       "         'us': 1,\n",
       "         'land': 1,\n",
       "         'saying': 1,\n",
       "         'closed': 1,\n",
       "         'sharply': 1,\n",
       "         'lawson': 1,\n",
       "         'received': 1,\n",
       "         'provide': 1,\n",
       "         'fuel': 1,\n",
       "         'caused': 1,\n",
       "         'arabia': 1,\n",
       "         'rising': 1,\n",
       "         'financing': 1,\n",
       "         'forecasts': 1,\n",
       "         'feet': 1,\n",
       "         'credits': 1,\n",
       "         'affected': 1,\n",
       "         'cooperation': 1,\n",
       "         'electric': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_table = []\n",
    "for v in vocabs:\n",
    "    uw = word_count[v] / num_total_words if v in word_count else 1/num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / z)\n",
    "    unigram_table.extend([v] * uw_alpha)\n",
    "\n",
    "Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15cbb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        target_index = targets[i].item()\n",
    "        nsample = []\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index: continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "    return torch.cat(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "819139ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size)\n",
    "\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1)\n",
    "        \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0237c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Skipgram Negative Sampling Training...\n",
      "Epoch 500 | Loss: 9.685333\n",
      "Epoch 1000 | Loss: 9.791967\n",
      "Epoch 1500 | Loss: 6.904815\n",
      "Epoch 2000 | Loss: 7.478949\n"
     ]
    }
   ],
   "source": [
    "# Training Setup\n",
    "model_neg = SkipgramNeg(voc_size, emb_size)\n",
    "optimizer_neg = optim.Adam(model_neg.parameters(), lr=0.001)\n",
    "k = 5\n",
    "\n",
    "print(\"\\nStarting Skipgram Negative Sampling Training...\")\n",
    "\n",
    "skipgrams = get_skipgrams_dynamic(corpus, window_size)\n",
    "\n",
    "start_neg = time.time()\n",
    "for epoch in range(2000):\n",
    "    input_batch, label_batch = random_batch(batch_size, skipgrams)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k)\n",
    "    loss = model_neg(input_tensor, label_tensor, neg_samples)\n",
    "    \n",
    "    optimizer_neg.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_neg.step()\n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "final_loss_neg = loss.item()\n",
    "time_neg = time.time() - start_neg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faa6c6",
   "metadata": {},
   "source": [
    "### 3. Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697056b",
   "metadata": {},
   "source": [
    "### GloVe (Global Vectors)\n",
    "\n",
    "GloVe learns word embeddings by factorizing a global word co-occurrence matrix.\n",
    "This allows GloVe to capture global statistical information from the corpus rather than only local context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c526b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def build_cooccurrence(corpus, window_size):\n",
    "    cooc = Counter()\n",
    "    for sentence in corpus:\n",
    "        for i, center in enumerate(sentence):\n",
    "            start = max(0, i - window_size)\n",
    "            end   = min(len(sentence), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                context = sentence[j]\n",
    "                cooc[(center, context)] += 1\n",
    "    return cooc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "abe56164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_random_batch(batch_size, cooc_counts, x_max=100, alpha=0.75):\n",
    "    pairs = list(cooc_counts.items())\n",
    "    batch = np.random.choice(len(pairs), batch_size)\n",
    "\n",
    "    centers, contexts, coocs, weights = [], [], [], []\n",
    "\n",
    "    for idx in batch:\n",
    "        (w_i, w_j), x_ij = pairs[idx]\n",
    "        centers.append(word2index[w_i])\n",
    "        contexts.append(word2index[w_j])\n",
    "        coocs.append(math.log(x_ij))\n",
    "        weights.append((x_ij / x_max) ** alpha if x_ij < x_max else 1.0)\n",
    "\n",
    "    return (\n",
    "        torch.LongTensor(centers),\n",
    "        torch.LongTensor(contexts),\n",
    "        torch.FloatTensor(coocs),\n",
    "        torch.FloatTensor(weights),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f35b2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.wi = nn.Embedding(voc_size, emb_size)\n",
    "        self.wj = nn.Embedding(voc_size, emb_size)\n",
    "        self.bi = nn.Embedding(voc_size, 1)\n",
    "        self.bj = nn.Embedding(voc_size, 1)\n",
    "\n",
    "    def forward(self, wi, wj, x_ij, weight):\n",
    "        vi = self.wi(wi)\n",
    "        vj = self.wj(wj)\n",
    "        bi = self.bi(wi).squeeze()\n",
    "        bj = self.bj(wj).squeeze()\n",
    "\n",
    "        dot = torch.sum(vi * vj, dim=1)\n",
    "        loss = weight * (dot + bi + bj - x_ij) ** 2\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a0d869fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_counts = build_cooccurrence(corpus, window_size)\n",
    "\n",
    "model_glove = Glove(voc_size, emb_size)\n",
    "optimizer_glove = optim.Adam(model_glove.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "19284037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training GloVe...\n",
      "Epoch 200 | Loss: 2.411132\n",
      "Epoch 400 | Loss: 2.414562\n",
      "Epoch 600 | Loss: 2.843925\n",
      "Epoch 800 | Loss: 5.122661\n",
      "Epoch 1000 | Loss: 2.523733\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining GloVe...\")\n",
    "\n",
    "start_glove = time.time()\n",
    "for epoch in range(1000):\n",
    "    wi, wj, xij, weight = glove_random_batch(batch_size, cooc_counts)\n",
    "\n",
    "    loss = model_glove(\n",
    "        torch.LongTensor(wi),\n",
    "        torch.LongTensor(wj),\n",
    "        torch.FloatTensor(xij),\n",
    "        torch.FloatTensor(weight)\n",
    "    )\n",
    "\n",
    "    optimizer_glove.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_glove.step()\n",
    "\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "final_loss_gv = loss.item()\n",
    "time_glove = time.time() - start_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa21ea",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7074b",
   "metadata": {},
   "source": [
    "## Task 2  Model Evaluation and Comparison\n",
    "\n",
    "The trained embeddings are evaluated using:\n",
    "- Training loss and time\n",
    "- Word analogy tasks (semantic and syntactic)\n",
    "- Word similarity scores (WordSim-353)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "604f707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import urllib.request\n",
    "import gensim.downloader as api\n",
    "\n",
    "# 1. ANALOGY SOLVER\n",
    "def solver(model, a, b, c, mode):\n",
    "    def get_vec(w):\n",
    "        idx = torch.LongTensor([word2index.get(w, word2index['<UNK>'])])\n",
    "        if mode == \"glove\":\n",
    "            # GloVe uses wi and wj\n",
    "            return (model.wi(idx) + model.wj(idx)).detach().squeeze().numpy()\n",
    "        else:\n",
    "            # Word2Vec models use embedding_center and embedding_outside\n",
    "            return (model.embedding_center(idx) + model.embedding_outside(idx)).detach().squeeze().numpy()\n",
    "\n",
    "    try:\n",
    "        vec_a, vec_b, vec_c = get_vec(a), get_vec(b), get_vec(c)\n",
    "        target = vec_b - vec_a + vec_c\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    best_word = None\n",
    "    best_sim = -1\n",
    "\n",
    "    for w in vocabs:\n",
    "        if w in [a, b, c]: continue\n",
    "        v_w = get_vec(w)\n",
    "        # Cosine similarity = 1 - cosine_distance\n",
    "        sim = 1 - cosine(target, v_w)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_word = w\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63de8c7",
   "metadata": {},
   "source": [
    "#### Note on Analogy Accuracy\n",
    "\n",
    "Analogy accuracy is relatively low due to:\n",
    "- Limited corpus size (Reuters-21578)\n",
    "- Smaller vocabulary compared to large datasets like Wikipedia\n",
    "- Removal of out-of-vocabulary words\n",
    "\n",
    "This behavior is expected and does not indicate an incorrect implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bf8ba100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, tests, mode):\n",
    "    if not tests: return 0.0\n",
    "    correct = 0\n",
    "    for a, b, c, d in tests:\n",
    "        if solver(model, a, b, c, mode) == d:\n",
    "            correct += 1\n",
    "    return (correct / len(tests)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2ba00864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LOAD DATA & FILTER (Ensure words exist in our small vocab)\n",
    "url = \"https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt\"\n",
    "urllib.request.urlretrieve(url, \"analogy.txt\")\n",
    "\n",
    "semantic_tests, syntactic_tests = [], []\n",
    "curr_cat = None\n",
    "\n",
    "with open(\"analogy.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\":\"):\n",
    "            curr_cat = line.strip()\n",
    "            continue\n",
    "        words = line.lower().split()\n",
    "        if len(words) == 4 and all(w in word2index for w in words):\n",
    "            if curr_cat == \": capital-common-countries\":\n",
    "                semantic_tests.append(words)\n",
    "            elif curr_cat == \": past-tense\":\n",
    "                syntactic_tests.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f8f5a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. GENISM BENCHMARK\n",
    "g_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "def gensim_accuracy(tests):\n",
    "    if not tests: return 0.0\n",
    "    correct = 0\n",
    "    for a, b, c, d in tests:\n",
    "        try:\n",
    "            pred = g_model.most_similar(positive=[b, c], negative=[a], topn=1)[0][0]\n",
    "            if pred == d: correct += 1\n",
    "        except: continue\n",
    "    return (correct / len(tests)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ce82476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================================================================\n",
      "Model              Win    Loss       Time       Syntactic %     Semantic %\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Skipgram           2      19.1729    469.71s    0.00            0.00\n",
      "Skipgram (NEG)     2      7.4789     301.02s    0.00            0.00\n",
      "GloVe              2      2.5237     46.42s     0.00            0.00\n",
      "GloVe (Gensim)     N/A    N/A        N/A        0.00            93.27\n"
     ]
    }
   ],
   "source": [
    "# 4. FINAL RESULTS TABLE\n",
    "results = [\n",
    "    [\"Skipgram\", window_size, f\"{final_loss_sg:.4f}\", f\"{time_sg:.2f}s\", \n",
    "     calculate_accuracy(model_sg, syntactic_tests, \"sg\"), \n",
    "     calculate_accuracy(model_sg, semantic_tests, \"sg\")],\n",
    "    \n",
    "    [\"Skipgram (NEG)\", window_size, f\"{final_loss_neg:.4f}\", f\"{time_neg:.2f}s\", \n",
    "     calculate_accuracy(model_neg, syntactic_tests, \"neg\"), \n",
    "     calculate_accuracy(model_neg, semantic_tests, \"neg\")],\n",
    "    \n",
    "    [\"GloVe\", window_size, f\"{final_loss_gv:.4f}\", f\"{time_glove:.2f}s\", \n",
    "     calculate_accuracy(model_glove, syntactic_tests, \"glove\"), \n",
    "     calculate_accuracy(model_glove, semantic_tests, \"glove\")],\n",
    "    \n",
    "    [\"GloVe (Gensim)\", \"N/A\", \"N/A\", \"N/A\", \n",
    "     gensim_accuracy(syntactic_tests), \n",
    "     gensim_accuracy(semantic_tests)]\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 95)\n",
    "print(f\"{'Model':<18} {'Win':<6} {'Loss':<10} {'Time':<10} {'Syntactic %':<15} {'Semantic %'}\")\n",
    "print(\"-\" * 95)\n",
    "for r in results:\n",
    "    print(f\"{r[0]:<18} {r[1]:<6} {r[2]:<10} {r[3]:<10} {r[4]:<15.2f} {r[5]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8b32d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# =========================================================================\n",
    "# 1. PREPARE LOOKUP DICTIONARIES (Fixed to match Task 1 attribute names)\n",
    "# =========================================================================\n",
    "def create_lookup(model, model_type='sg'):\n",
    "    if model_type == 'glove':\n",
    "        v = model.wi.weight.detach()\n",
    "        u = model.wj.weight.detach()\n",
    "    else:\n",
    "        v = model.embedding_center.weight.detach()\n",
    "        u = model.embedding_outside.weight.detach()\n",
    "\n",
    "    W = (v + u) / 2\n",
    "    Wn = W / (W.norm(p=2, dim=1, keepdim=True) + 1e-9)\n",
    "\n",
    "    return {\"stoi\": word2index, \"Wn\": Wn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3addfdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookups for our trained models\n",
    "skipgram_lookup = create_lookup(model_sg, 'sg')\n",
    "skipgram_neg_lookup = create_lookup(model_neg, 'sg')\n",
    "glove_lookup = create_lookup(model_glove, 'glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000c3b66",
   "metadata": {},
   "source": [
    "### Word Similarity Evaluation (WordSim-353)\n",
    "\n",
    "Word similarity is evaluated using cosine similarity between word vectors.\n",
    "Spearman correlation measures how well the model's similarity rankings align with human judgments.\n",
    "Mean Squared Error (MSE) is also reported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "199a2bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 354 word pairs.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 2. LOAD WORDSIM353 DATASET\n",
    "# =========================================================================\n",
    "ws_path = datapath(\"wordsim353.tsv\")\n",
    "with open(ws_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    lines = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "rows = []\n",
    "for ln in lines:\n",
    "    parts = ln.split()\n",
    "    # Check if we have at least 3 parts (Word1, Word2, Score)\n",
    "    if len(parts) < 3: \n",
    "        continue\n",
    "    \n",
    "    # Use a try-except block to skip metadata/header lines\n",
    "    try:\n",
    "        w1 = parts[0].lower()\n",
    "        w2 = parts[1].lower()\n",
    "        score = float(parts[2]) # This will fail on text like 'WordSimilarity-353'\n",
    "        rows.append((w1, w2, score))\n",
    "    except ValueError:\n",
    "        # This skips the line if parts[2] is not a number\n",
    "        continue\n",
    "\n",
    "ws = pd.DataFrame(rows, columns=[\"Word 1\", \"Word 2\", \"Human (mean)\"])\n",
    "print(f\"Successfully loaded {len(ws)} word pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8749630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# 3. SIMILARITY SCORE FUNCTIONS\n",
    "# =========================================================================\n",
    "def similarity_scores_torch(lookup, ws_df):\n",
    "    stoi_local = lookup[\"stoi\"]\n",
    "    Wn = lookup[\"Wn\"]\n",
    "    sims, gold, skipped = [], [], 0\n",
    "\n",
    "    for _, row in ws_df.iterrows():\n",
    "        w1, w2, score = row[\"Word 1\"], row[\"Word 2\"], row[\"Human (mean)\"]\n",
    "        if w1 not in stoi_local or w2 not in stoi_local:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        v1, v2 = Wn[stoi_local[w1]], Wn[stoi_local[w2]]\n",
    "        # Dot product of normalized vectors = Cosine Similarity\n",
    "        sims.append(torch.dot(v1, v2).item())\n",
    "        gold.append(score)\n",
    "    return np.array(sims), np.array(gold), skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "57e06d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_scores_gensim(model, ws_df):\n",
    "    sims, gold, skipped = [], [], 0\n",
    "    for _, row in ws_df.iterrows():\n",
    "        w1, w2, score = row[\"Word 1\"], row[\"Word 2\"], row[\"Human (mean)\"]\n",
    "        if w1 not in model or w2 not in model:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        sims.append(model.similarity(w1, w2))\n",
    "        gold.append(score)\n",
    "    return np.array(sims), np.array(gold), skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ee759f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# 4. CALCULATE METRICS\n",
    "# =========================================================================\n",
    "results_similarity = []\n",
    "\n",
    "# Eval loop for custom models\n",
    "for name, lookup in [(\"Skipgram\", skipgram_lookup), (\"Skipgram (NEG)\", skipgram_neg_lookup), (\"GloVe\", glove_lookup)]:\n",
    "    sims, gold, skipped = similarity_scores_torch(lookup, ws)\n",
    "    rho, _ = spearmanr(sims, gold)\n",
    "    # Human scores are 0-10, sims are -1 to 1. To calculate MSE, we normalize human scores to 0-1\n",
    "    mse = np.mean(((sims) - (gold/10)) ** 2) \n",
    "    results_similarity.append({\"Model\": name, \"Spearman\": rho, \"MSE\": mse, \"Skipped\": skipped})\n",
    "\n",
    "# Eval for Gensim\n",
    "sims, gold, skipped = similarity_scores_gensim(g_model, ws)\n",
    "rho, _ = spearmanr(sims, gold)\n",
    "mse = np.mean(((sims) - (gold/10)) ** 2)\n",
    "results_similarity.append({\"Model\": \"GloVe (Gensim)\", \"Spearman\": rho, \"MSE\": mse, \"Skipped\": skipped})\n",
    "\n",
    "sim_df = pd.DataFrame(results_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93542e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table 1. Swapped Columns and Rows Table\n",
      "Model     Skipgram  Skipgram (NEG)  GloVe  GloVe (Gensim)\n",
      "Spearman     0.021           0.052  0.084           0.536\n",
      "MSE          0.388           0.385  0.417           0.053\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 5. FINAL MERGED TABLE & TABLE 1 (SWAPPED)\n",
    "# =========================================================================\n",
    "table_required = sim_df.set_index(\"Model\")[[\"Spearman\", \"MSE\"]].T\n",
    "\n",
    "table_required = table_required.round(3)\n",
    "\n",
    "print(\"\\nTable 1. Swapped Columns and Rows Table\")\n",
    "print(table_required)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11476923",
   "metadata": {},
   "source": [
    "## TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5f8353b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: model_sg.pth, model_neg.pth, model_glove.pth, model_data.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 1. Save all 3 Model Weights\n",
    "torch.save(model_sg.state_dict(), 'model_sg.pth')\n",
    "torch.save(model_neg.state_dict(), 'model_neg.pth')\n",
    "torch.save(model_glove.state_dict(), 'model_glove.pth')\n",
    "\n",
    "# 2. Save Metadata\n",
    "data_to_save = {\n",
    "    'word2index': word2index,\n",
    "    'voc_size': voc_size,\n",
    "    'emb_size': emb_size,\n",
    "    'corpus_raw': corpus,\n",
    "    'corpus_tokens': corpus\n",
    "}\n",
    "\n",
    "with open('model_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)\n",
    "    print(\"Exported: model_sg.pth, model_neg.pth, model_glove.pth, model_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c20910",
   "metadata": {},
   "source": [
    "## Final Observations\n",
    "\n",
    "Three word embedding models were trained from scratch using Reuters news data: regular Skip-gram, Skip-gram with negative sampling, and GloVe.\n",
    "\n",
    "Negative sampling proved clearly faster and more effective than full softmax  training time decreased and loss dropped more smoothly. GloVe trained quickest and reached the lowest final loss, most likely because global co-occurrence statistics were used.\n",
    "\n",
    "Analogy scores (both syntactic and semantic) reached 0%. This outcome is expected. Reuters corpus remains small and focuses heavily on finance and business news, so general word relationships like king  queen or verb tenses cannot be learned well. Assignment instructions already mentioned that 0% accuracy should not be surprising due to corpus limitations.\n",
    "\n",
    "WordSim-353 similarity test showed very low correlation (around 0.020.08) across all trained models. This result is reasonable since training data differs greatly from general words used in human similarity ratings. For comparison, large pre-trained GloVe from Gensim achieved much higher correlation (~0.54), which demonstrates importance of corpus size and variety.\n",
    "\n",
    "Training process clearly showed differences between methods: Word2Vec focuses on nearby words while GloVe considers overall count statistics. Even though benchmark numbers stay modest, complete pipeline worked successfully and real challenges of training on small, specialized corpus became visible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

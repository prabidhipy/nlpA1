{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad1af6a",
   "metadata": {},
   "source": [
    "## TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79bba4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a9354e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/prabidhi/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/prabidhi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#loading reuters dataset\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def load_reuters():\n",
    "    fileids = reuters.fileids()\n",
    "    corpus = []\n",
    "    for fid in fileids:\n",
    "        words = [w.lower() for w in reuters.words(fid)]\n",
    "        words = [re.sub(r\"[^a-z]\", \"\", w) for w in words]\n",
    "        words = [w for w in words if w]\n",
    "        if len(words) > 5:\n",
    "            corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "corpus = load_reuters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b57d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7efde9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Building\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus)))\n",
    "vocabs.append('<UNK>')\n",
    "word2index = {v: idx for idx, v in enumerate(vocabs)}\n",
    "index2word = {idx: v for v, idx in word2index.items()}\n",
    "voc_size = len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff5bc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for sequences\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dynamic window size\n",
    "def get_skipgrams_dynamic(corpus, max_window=2):\n",
    "    skipgrams = []\n",
    "    for doc in corpus:\n",
    "        for i in range(len(doc)):\n",
    "            w = random.randint(1, max_window)\n",
    "            for j in range(-w, w + 1):\n",
    "                if j == 0 or i + j < 0 or i + j >= len(doc):\n",
    "                    continue\n",
    "                skipgrams.append([\n",
    "                    word2index.get(doc[i], word2index['<UNK>']),\n",
    "                    word2index.get(doc[i+j], word2index['<UNK>'])\n",
    "                ])\n",
    "    return skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e48a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, skipgrams):\n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "    return np.array(inputs), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6647c1",
   "metadata": {},
   "source": [
    "### 1. Word2Vec - Without Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center) \n",
    "        outside_embedding = self.embedding_outside(outside)\n",
    "        all_vocabs_embedding = self.embedding_outside(all_vocabs)\n",
    "\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1).reshape(-1, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfdf33a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Skipgram Without Negative Sampling\n",
      "Epoch 500 | Loss: 23.074251\n",
      "Epoch 1000 | Loss: 19.695658\n",
      "Epoch 1500 | Loss: 17.679941\n",
      "Epoch 2000 | Loss: 18.475311\n"
     ]
    }
   ],
   "source": [
    "# Training Setup\n",
    "emb_size = 50\n",
    "batch_size = 64\n",
    "window_size = 2 # DYNAMIC WINDOW SIZE\n",
    "# skipgrams = get_skipgrams_dynamic(corpus, window_size)\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "\n",
    "model_sg = Skipgram(voc_size, emb_size)\n",
    "optimizer_sg = optim.Adam(model_sg.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Starting Skipgram Without Negative Sampling\")\n",
    "skipgrams = get_skipgrams_dynamic(corpus, window_size)\n",
    "\n",
    "start_sg = time.time()\n",
    "for epoch in range(2000):\n",
    "    input_batch, label_batch = random_batch(batch_size, skipgrams)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    loss = model_sg(input_tensor, label_tensor, all_vocabs)\n",
    "\n",
    "    optimizer_sg.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_sg.step()\n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "final_loss_sg = loss.item()\n",
    "time_sg = time.time() - start_sg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e76164",
   "metadata": {},
   "source": [
    "### 2. Word2Vec - Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15cbb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram distribution for negative sampling\n",
    "z = 0.001\n",
    "word_count = Counter(flatten(corpus))\n",
    "num_total_words = sum(word_count.values())\n",
    "unigram_table = []\n",
    "for v in vocabs:\n",
    "    uw = word_count[v] / num_total_words if v in word_count else 1/num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / z)\n",
    "    unigram_table.extend([v] * uw_alpha)\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        target_index = targets[i].item()\n",
    "        nsample = []\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index: continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "    return torch.cat(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0237c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Skipgram Negative Sampling Training...\n",
      "Epoch 500 | Loss: 16.214039\n",
      "Epoch 1000 | Loss: 13.803610\n",
      "Epoch 1500 | Loss: 13.728167\n",
      "Epoch 2000 | Loss: 11.222450\n"
     ]
    }
   ],
   "source": [
    "# Training Setup\n",
    "model_neg = SkipgramNeg(voc_size, emb_size)\n",
    "optimizer_neg = optim.Adam(model_neg.parameters(), lr=0.001)\n",
    "k = 5\n",
    "\n",
    "print(\"\\nStarting Skipgram Negative Sampling Training...\")\n",
    "\n",
    "skipgrams = get_skipgrams_dynamic(corpus, window_size)\n",
    "\n",
    "start_neg = time.time()\n",
    "for epoch in range(2000):\n",
    "    input_batch, label_batch = random_batch(batch_size, skipgrams)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k)\n",
    "    loss = model_neg(input_tensor, label_tensor, neg_samples)\n",
    "    \n",
    "    optimizer_neg.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_neg.step()\n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "final_loss_neg = loss.item()\n",
    "time_neg = time.time() - start_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c526b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def build_cooccurrence(corpus, window_size):\n",
    "    cooc = Counter()\n",
    "    for sentence in corpus:\n",
    "        for i, center in enumerate(sentence):\n",
    "            start = max(0, i - window_size)\n",
    "            end   = min(len(sentence), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                context = sentence[j]\n",
    "                cooc[(center, context)] += 1\n",
    "    return cooc\n",
    "\n",
    "def glove_random_batch(batch_size, cooc_counts, x_max=100, alpha=0.75):\n",
    "    pairs = list(cooc_counts.items())\n",
    "    batch = np.random.choice(len(pairs), batch_size)\n",
    "\n",
    "    centers, contexts, coocs, weights = [], [], [], []\n",
    "\n",
    "    for idx in batch:\n",
    "        (w_i, w_j), x_ij = pairs[idx]\n",
    "        centers.append(word2index[w_i])\n",
    "        contexts.append(word2index[w_j])\n",
    "        coocs.append(math.log(x_ij))\n",
    "        weights.append((x_ij / x_max) ** alpha if x_ij < x_max else 1.0)\n",
    "\n",
    "    return (\n",
    "        torch.LongTensor(centers),\n",
    "        torch.LongTensor(contexts),\n",
    "        torch.FloatTensor(coocs),\n",
    "        torch.FloatTensor(weights),\n",
    "    )\n",
    "\n",
    "class Glove(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.wi = nn.Embedding(voc_size, emb_size)\n",
    "        self.wj = nn.Embedding(voc_size, emb_size)\n",
    "        self.bi = nn.Embedding(voc_size, 1)\n",
    "        self.bj = nn.Embedding(voc_size, 1)\n",
    "\n",
    "    def forward(self, wi, wj, x_ij, weight):\n",
    "        vi = self.wi(wi)\n",
    "        vj = self.wj(wj)\n",
    "        bi = self.bi(wi).squeeze()\n",
    "        bj = self.bj(wj).squeeze()\n",
    "\n",
    "        dot = torch.sum(vi * vj, dim=1)\n",
    "        loss = weight * (dot + bi + bj - x_ij) ** 2\n",
    "        return torch.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0d869fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training GloVe...\n",
      "Epoch 200 | Loss: 2.444613\n",
      "Epoch 400 | Loss: 3.370449\n",
      "Epoch 600 | Loss: 1.964923\n",
      "Epoch 800 | Loss: 4.077344\n",
      "Epoch 1000 | Loss: 3.128252\n"
     ]
    }
   ],
   "source": [
    "cooc_counts = build_cooccurrence(corpus, window_size)\n",
    "\n",
    "model_glove = Glove(voc_size, emb_size)\n",
    "optimizer_glove = optim.Adam(model_glove.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining GloVe...\")\n",
    "\n",
    "start_glove = time.time()\n",
    "for epoch in range(1000):\n",
    "    wi, wj, xij, weight = glove_random_batch(batch_size, cooc_counts)\n",
    "\n",
    "    loss = model_glove(\n",
    "        torch.LongTensor(wi),\n",
    "        torch.LongTensor(wj),\n",
    "        torch.FloatTensor(xij),\n",
    "        torch.FloatTensor(weight)\n",
    "    )\n",
    "\n",
    "    optimizer_glove.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_glove.step()\n",
    "\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "final_loss_gv = loss.item()\n",
    "time_glove = time.time() - start_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa21ea",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "604f707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================================================================\n",
      "Model              Win    Loss       Time       Syntactic %     Semantic %\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Skipgram           2      18.4753    493.76s    0.00            0.29\n",
      "Skipgram (NEG)     2      11.2225    316.33s    0.00            0.00\n",
      "GloVe              2      3.1283     50.46s     0.00            0.00\n",
      "GloVe (Gensim)     N/A    N/A        N/A        0.00            93.27\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import urllib.request\n",
    "import gensim.downloader as api\n",
    "\n",
    "# 1. ANALOGY SOLVER (Fixed Attribute Names)\n",
    "def solver(model, a, b, c, mode):\n",
    "    def get_vec(w):\n",
    "        idx = torch.LongTensor([word2index.get(w, word2index['<UNK>'])])\n",
    "        if mode == \"glove\":\n",
    "            # GloVe uses wi and wj\n",
    "            return (model.wi(idx) + model.wj(idx)).detach().squeeze().numpy()\n",
    "        else:\n",
    "            # Word2Vec models use embedding_center and embedding_outside\n",
    "            return (model.embedding_center(idx) + model.embedding_outside(idx)).detach().squeeze().numpy()\n",
    "\n",
    "    try:\n",
    "        vec_a, vec_b, vec_c = get_vec(a), get_vec(b), get_vec(c)\n",
    "        target = vec_b - vec_a + vec_c\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    best_word = None\n",
    "    best_sim = -1\n",
    "\n",
    "    for w in vocabs:\n",
    "        if w in [a, b, c]: continue\n",
    "        v_w = get_vec(w)\n",
    "        # Cosine similarity = 1 - cosine_distance\n",
    "        sim = 1 - cosine(target, v_w)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_word = w\n",
    "    return best_word\n",
    "\n",
    "def calculate_accuracy(model, tests, mode):\n",
    "    if not tests: return 0.0\n",
    "    correct = 0\n",
    "    for a, b, c, d in tests:\n",
    "        if solver(model, a, b, c, mode) == d:\n",
    "            correct += 1\n",
    "    return (correct / len(tests)) * 100\n",
    "\n",
    "# 2. LOAD DATA & FILTER (Ensure words exist in our small vocab)\n",
    "url = \"https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt\"\n",
    "urllib.request.urlretrieve(url, \"analogy.txt\")\n",
    "\n",
    "semantic_tests, syntactic_tests = [], []\n",
    "curr_cat = None\n",
    "\n",
    "with open(\"analogy.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\":\"):\n",
    "            curr_cat = line.strip()\n",
    "            continue\n",
    "        words = line.lower().split()\n",
    "        if len(words) == 4 and all(w in word2index for w in words):\n",
    "            if curr_cat == \": capital-common-countries\":\n",
    "                semantic_tests.append(words)\n",
    "            elif curr_cat == \": past-tense\":\n",
    "                syntactic_tests.append(words)\n",
    "\n",
    "# 3. GENISM BENCHMARK\n",
    "g_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "def gensim_accuracy(tests):\n",
    "    if not tests: return 0.0\n",
    "    correct = 0\n",
    "    for a, b, c, d in tests:\n",
    "        try:\n",
    "            pred = g_model.most_similar(positive=[b, c], negative=[a], topn=1)[0][0]\n",
    "            if pred == d: correct += 1\n",
    "        except: continue\n",
    "    return (correct / len(tests)) * 100\n",
    "\n",
    "# 4. FINAL RESULTS TABLE\n",
    "results = [\n",
    "    [\"Skipgram\", window_size, f\"{final_loss_sg:.4f}\", f\"{time_sg:.2f}s\", \n",
    "     calculate_accuracy(model_sg, syntactic_tests, \"sg\"), \n",
    "     calculate_accuracy(model_sg, semantic_tests, \"sg\")],\n",
    "    \n",
    "    [\"Skipgram (NEG)\", window_size, f\"{final_loss_neg:.4f}\", f\"{time_neg:.2f}s\", \n",
    "     calculate_accuracy(model_neg, syntactic_tests, \"neg\"), \n",
    "     calculate_accuracy(model_neg, semantic_tests, \"neg\")],\n",
    "    \n",
    "    [\"GloVe\", window_size, f\"{final_loss_gv:.4f}\", f\"{time_glove:.2f}s\", \n",
    "     calculate_accuracy(model_glove, syntactic_tests, \"glove\"), \n",
    "     calculate_accuracy(model_glove, semantic_tests, \"glove\")],\n",
    "    \n",
    "    [\"GloVe (Gensim)\", \"N/A\", \"N/A\", \"N/A\", \n",
    "     gensim_accuracy(syntactic_tests), \n",
    "     gensim_accuracy(semantic_tests)]\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 95)\n",
    "print(f\"{'Model':<18} {'Win':<6} {'Loss':<10} {'Time':<10} {'Syntactic %':<15} {'Semantic %'}\")\n",
    "print(\"-\" * 95)\n",
    "for r in results:\n",
    "    print(f\"{r[0]:<18} {r[1]:<6} {r[2]:<10} {r[3]:<10} {r[4]:<15.2f} {r[5]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b32d5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 354 word pairs.\n",
      "\n",
      "Table 1. Swapped Columns and Rows Table\n",
      "Model     Skipgram  Skipgram (NEG)  GloVe  GloVe (Gensim)\n",
      "Spearman     0.002           0.066  0.005           0.536\n",
      "MSE          0.402           0.393  0.400           0.053\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# =========================================================================\n",
    "# 1. PREPARE LOOKUP DICTIONARIES (Fixed to match Task 1 attribute names)\n",
    "# =========================================================================\n",
    "def create_lookup(model, model_type='sg'):\n",
    "    if model_type == 'glove':\n",
    "        v = model.wi.weight.detach()\n",
    "        u = model.wj.weight.detach()\n",
    "    else:\n",
    "        v = model.embedding_center.weight.detach()\n",
    "        u = model.embedding_outside.weight.detach()\n",
    "\n",
    "    W = (v + u) / 2\n",
    "    Wn = W / (W.norm(p=2, dim=1, keepdim=True) + 1e-9)\n",
    "\n",
    "    return {\"stoi\": word2index, \"Wn\": Wn}\n",
    "\n",
    "# Create lookups for our trained models\n",
    "skipgram_lookup = create_lookup(model_sg, 'sg')\n",
    "skipgram_neg_lookup = create_lookup(model_neg, 'sg')\n",
    "glove_lookup = create_lookup(model_glove, 'glove')\n",
    "\n",
    "# =========================================================================\n",
    "# 2. LOAD WORDSIM353 DATASET (Corrected with Error Handling)\n",
    "# =========================================================================\n",
    "ws_path = datapath(\"wordsim353.tsv\")\n",
    "with open(ws_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    lines = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "rows = []\n",
    "for ln in lines:\n",
    "    parts = ln.split()\n",
    "    # Check if we have at least 3 parts (Word1, Word2, Score)\n",
    "    if len(parts) < 3: \n",
    "        continue\n",
    "    \n",
    "    # Use a try-except block to skip metadata/header lines\n",
    "    try:\n",
    "        w1 = parts[0].lower()\n",
    "        w2 = parts[1].lower()\n",
    "        score = float(parts[2]) # This will fail on text like 'WordSimilarity-353'\n",
    "        rows.append((w1, w2, score))\n",
    "    except ValueError:\n",
    "        # This skips the line if parts[2] is not a number\n",
    "        continue\n",
    "\n",
    "ws = pd.DataFrame(rows, columns=[\"Word 1\", \"Word 2\", \"Human (mean)\"])\n",
    "print(f\"Successfully loaded {len(ws)} word pairs.\")\n",
    "\n",
    "# =========================================================================\n",
    "# 3. SIMILARITY SCORE FUNCTIONS\n",
    "# =========================================================================\n",
    "def similarity_scores_torch(lookup, ws_df):\n",
    "    stoi_local = lookup[\"stoi\"]\n",
    "    Wn = lookup[\"Wn\"]\n",
    "    sims, gold, skipped = [], [], 0\n",
    "\n",
    "    for _, row in ws_df.iterrows():\n",
    "        w1, w2, score = row[\"Word 1\"], row[\"Word 2\"], row[\"Human (mean)\"]\n",
    "        if w1 not in stoi_local or w2 not in stoi_local:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        v1, v2 = Wn[stoi_local[w1]], Wn[stoi_local[w2]]\n",
    "        # Dot product of normalized vectors = Cosine Similarity\n",
    "        sims.append(torch.dot(v1, v2).item())\n",
    "        gold.append(score)\n",
    "    return np.array(sims), np.array(gold), skipped\n",
    "\n",
    "def similarity_scores_gensim(model, ws_df):\n",
    "    sims, gold, skipped = [], [], 0\n",
    "    for _, row in ws_df.iterrows():\n",
    "        w1, w2, score = row[\"Word 1\"], row[\"Word 2\"], row[\"Human (mean)\"]\n",
    "        if w1 not in model or w2 not in model:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        sims.append(model.similarity(w1, w2))\n",
    "        gold.append(score)\n",
    "    return np.array(sims), np.array(gold), skipped\n",
    "\n",
    "# =========================================================================\n",
    "# 4. CALCULATE METRICS\n",
    "# =========================================================================\n",
    "results_similarity = []\n",
    "\n",
    "# Eval loop for custom models\n",
    "for name, lookup in [(\"Skipgram\", skipgram_lookup), (\"Skipgram (NEG)\", skipgram_neg_lookup), (\"GloVe\", glove_lookup)]:\n",
    "    sims, gold, skipped = similarity_scores_torch(lookup, ws)\n",
    "    rho, _ = spearmanr(sims, gold)\n",
    "    # Human scores are 0-10, sims are -1 to 1. To calculate MSE, we normalize human scores to 0-1\n",
    "    mse = np.mean(((sims) - (gold/10)) ** 2) \n",
    "    results_similarity.append({\"Model\": name, \"Spearman\": rho, \"MSE\": mse, \"Skipped\": skipped})\n",
    "\n",
    "# Eval for Gensim\n",
    "sims, gold, skipped = similarity_scores_gensim(g_model, ws)\n",
    "rho, _ = spearmanr(sims, gold)\n",
    "mse = np.mean(((sims) - (gold/10)) ** 2)\n",
    "results_similarity.append({\"Model\": \"GloVe (Gensim)\", \"Spearman\": rho, \"MSE\": mse, \"Skipped\": skipped})\n",
    "\n",
    "sim_df = pd.DataFrame(results_similarity)\n",
    "\n",
    "# =========================================================================\n",
    "# 5. FINAL MERGED TABLE & TABLE 1 (SWAPPED)\n",
    "# =========================================================================\n",
    "table_required = sim_df.set_index(\"Model\")[[\"Spearman\", \"MSE\"]].T\n",
    "\n",
    "# Optional rounding (already done, but safe)\n",
    "table_required = table_required.round(3)\n",
    "\n",
    "print(\"\\nTable 1. Swapped Columns and Rows Table\")\n",
    "print(table_required)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11476923",
   "metadata": {},
   "source": [
    "## TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f8353b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: model_sg.pth, model_neg.pth, model_glove.pth, model_data.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 1. Save all 3 Model Weights\n",
    "torch.save(model_sg.state_dict(), 'model_sg.pth')\n",
    "torch.save(model_neg.state_dict(), 'model_neg.pth')\n",
    "torch.save(model_glove.state_dict(), 'model_glove.pth')\n",
    "\n",
    "# 2. Save Metadata\n",
    "data_to_save = {\n",
    "    'word2index': word2index,\n",
    "    'voc_size': voc_size,\n",
    "    'emb_size': emb_size,\n",
    "    'corpus_raw': corpus,\n",
    "    'corpus_tokens': corpus\n",
    "}\n",
    "\n",
    "with open('model_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)\n",
    "    print(\"Exported: model_sg.pth, model_neg.pth, model_glove.pth, model_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7dbc0e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"oil\" in word2index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25f7a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e303c",
   "metadata": {},
   "source": [
    "## TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f971b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"reutersNLTK.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91b6aea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>categories</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/14826</td>\n",
       "      <td>['trade']</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/14828</td>\n",
       "      <td>['grain']</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/14829</td>\n",
       "      <td>['crude', 'nat-gas']</td>\n",
       "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/14832</td>\n",
       "      <td>['corn', 'grain', 'rice', 'rubber', 'sugar', '...</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/14833</td>\n",
       "      <td>['palm-oil', 'veg-oil']</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ids                                         categories  \\\n",
       "0  test/14826                                          ['trade']   \n",
       "1  test/14828                                          ['grain']   \n",
       "2  test/14829                               ['crude', 'nat-gas']   \n",
       "3  test/14832  ['corn', 'grain', 'rice', 'rubber', 'sugar', '...   \n",
       "4  test/14833                            ['palm-oil', 'veg-oil']   \n",
       "\n",
       "                                                text  \n",
       "0  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...  \n",
       "1  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...  \n",
       "2  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...  \n",
       "3  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...  \n",
       "4  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8deb1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a subset for faster training demonstration, but you can increase this\n",
    "corpus_raw = df['text'].astype(str).tolist()[:100] \n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "    processed = []\n",
    "    for text in corpus:\n",
    "        # Lowercase, remove special characters and extra spaces\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        tokens = text.split()\n",
    "        if len(tokens) > 5: # Only keep sentences with enough context\n",
    "            processed.append(tokens)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8462a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocess_corpus(corpus_raw)\n",
    "\n",
    "# Vocabulary Building\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus)))\n",
    "vocabs.append('<UNK>')\n",
    "word2index = {v: idx for idx, v in enumerate(vocabs)}\n",
    "index2word = {idx: v for v, idx in word2index.items()}\n",
    "voc_size = len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d86a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for sequences\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c3a7c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TASK REQUIREMENT: DYNAMIC WINDOW SIZE ---\n",
    "def get_skipgrams(corpus, window_size=2):\n",
    "    skipgrams = []\n",
    "    for doc in corpus:\n",
    "        for i in range(window_size, len(doc) - window_size):\n",
    "            center = word2index.get(doc[i], word2index['<UNK>'])\n",
    "            # Extract context within window_size\n",
    "            for j in range(-window_size, window_size + 1):\n",
    "                if j == 0: continue # skip center word\n",
    "                outside = word2index.get(doc[i+j], word2index['<UNK>'])\n",
    "                skipgrams.append([center, outside])\n",
    "    return skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54213437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, skipgrams):\n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "    return np.array(inputs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827761e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Skipgram Vanilla Training...\n",
      "Epoch 500 | Loss: 8.692552\n",
      "Epoch 1000 | Loss: 8.518382\n",
      "Epoch 1500 | Loss: 8.141383\n",
      "Epoch 2000 | Loss: 8.175533\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 1. Word2Vec (Without Negative Sampling)\n",
    "# =========================================================================\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center) \n",
    "        outside_embedding    = self.embedding_center(outside) \n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) \n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1).reshape(-1, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))\n",
    "        return loss\n",
    "\n",
    "# Training Setup\n",
    "emb_size = 2\n",
    "batch_size = 64\n",
    "window_size = 2 # DYNAMIC WINDOW SIZE\n",
    "skipgrams = get_skipgrams(corpus, window_size)\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "\n",
    "model_sg = Skipgram(voc_size, emb_size)\n",
    "optimizer_sg = optim.Adam(model_sg.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Starting Skipgram Without Negative Sampling\")\n",
    "for epoch in range(2000):\n",
    "    input_batch, label_batch = random_batch(batch_size, skipgrams)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    loss = model_sg(input_tensor, label_tensor, all_vocabs)\n",
    "    optimizer_sg.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_sg.step()\n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss.item():2.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4479408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Skipgram Negative Sampling Training...\n",
      "Epoch 500 | Loss: 2.324990\n",
      "Epoch 1000 | Loss: 2.142023\n",
      "Epoch 1500 | Loss: 1.816248\n",
      "Epoch 2000 | Loss: 1.850167\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 2. Word2Vec (Negative Sampling)\n",
    "# =========================================================================\n",
    "\n",
    "# Unigram distribution for negative sampling\n",
    "z = 0.001\n",
    "word_count = Counter(flatten(corpus))\n",
    "num_total_words = sum(word_count.values())\n",
    "unigram_table = []\n",
    "for v in vocabs:\n",
    "    uw = word_count[v] / num_total_words if v in word_count else 1/num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / z)\n",
    "    unigram_table.extend([v] * uw_alpha)\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        target_index = targets[i].item()\n",
    "        nsample = []\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index: continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "    return torch.cat(neg_samples)\n",
    "\n",
    "class SkipgramNeg(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        center_embed   = self.embedding_center(center) \n",
    "        outside_embed  = self.embedding_outside(outside) \n",
    "        negative_embed = self.embedding_outside(negative) \n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) \n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) \n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) \n",
    "        \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        return -torch.mean(loss)\n",
    "\n",
    "# Training Setup\n",
    "model_neg = SkipgramNeg(voc_size, emb_size)\n",
    "optimizer_neg = optim.Adam(model_neg.parameters(), lr=0.001)\n",
    "k = 5\n",
    "\n",
    "print(\"\\nStarting Skipgram Negative Sampling Training...\")\n",
    "for epoch in range(2000):\n",
    "    input_batch, label_batch = random_batch(batch_size, skipgrams)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k)\n",
    "    loss = model_neg(input_tensor, label_tensor, neg_samples)\n",
    "    \n",
    "    optimizer_neg.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_neg.step()\n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss.item():2.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8221ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from gensim) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from gensim) (1.16.1)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Downloading wrapt-2.0.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Downloading gensim-4.4.0-cp313-cp313-macosx_11_0_arm64.whl (24.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.4/24.4 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading wrapt-2.0.1-cp313-cp313-macosx_11_0_arm64.whl (61 kB)\n",
      "Installing collected packages: wrapt, smart_open, gensim\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [gensim]2m2/3\u001b[0m [gensim]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gensim-4.4.0 smart_open-7.5.0 wrapt-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8972b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Skipgram (Vanilla)...\n",
      "Training Skipgram (Negative Sampling)...\n",
      "Training GloVe...\n",
      "Loading Gensim GloVe...\n",
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "\n",
      "=====================================================================================\n",
      "Model              Win   Loss       Time       Syntactic %     Semantic %\n",
      "-------------------------------------------------------------------------------------\n",
      "Skipgram           2     8.7263     7.86s      0.00            0.00\n",
      "Skipgram (NEG)     2     1.9949     3.78s      0.00            0.00\n",
      "Glove              2     7.6050     0.48s      0.00            0.00\n",
      "Glove (Gensim)     N/A   N/A        N/A        0.00            80.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cosine\n",
    "import urllib.request\n",
    "import gensim.downloader as api\n",
    "\n",
    "# =========================================================================\n",
    "# 0. DATA PREPARATION (Shared for all models)\n",
    "# =========================================================================\n",
    "df = pd.read_excel(\"reutersNLTK.xlsx\")\n",
    "\n",
    "# Use a subset for training speed; increase this for better accuracy\n",
    "corpus_raw = df['text'].astype(str).tolist()[:200] \n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "    processed = []\n",
    "    for text in corpus:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        tokens = text.split()\n",
    "        if len(tokens) > 5:\n",
    "            processed.append(tokens)\n",
    "    return processed\n",
    "\n",
    "corpus = preprocess_corpus(corpus_raw)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus)))\n",
    "vocabs.append('<UNK>')\n",
    "word2index = {v: idx for idx, v in enumerate(vocabs)}\n",
    "index2word = {idx: v for v, idx in word2index.items()}\n",
    "voc_size = len(vocabs)\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "# --- DYNAMIC WINDOW SIZE LOGIC ---\n",
    "def get_skipgrams(corpus, window_size=2):\n",
    "    skipgrams = []\n",
    "    for doc in corpus:\n",
    "        for i in range(len(doc)):\n",
    "            center = word2index.get(doc[i], word2index['<UNK>'])\n",
    "            # Dynamic Window: look left and right\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(doc), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i == j: continue\n",
    "                outside = word2index.get(doc[j], word2index['<UNK>'])\n",
    "                skipgrams.append([center, outside])\n",
    "    return skipgrams\n",
    "\n",
    "def random_batch(batch_size, skipgrams):\n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "    return np.array(inputs), np.array(labels)\n",
    "\n",
    "# Global Config\n",
    "window_size = 2 # Default window size as requested\n",
    "batch_size = 64\n",
    "emb_size = 2\n",
    "skipgrams = get_skipgrams(corpus, window_size)\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Word2Vec (Skipgram Vanilla)\n",
    "# =========================================================================\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center) \n",
    "        outside_embedding    = self.embedding_center(outside) \n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1).reshape(-1, 1)\n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))\n",
    "        return loss\n",
    "\n",
    "model_sg = Skipgram(voc_size, emb_size)\n",
    "optimizer_sg = optim.Adam(model_sg.parameters(), lr=0.001)\n",
    "all_vocabs_tensor = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "\n",
    "print(\"Training Skipgram (Vanilla)...\")\n",
    "start_sg = time.time()\n",
    "for epoch in range(1000):\n",
    "    input_batch, label_batch = random_batch(batch_size, skipgrams)\n",
    "    loss_sg = model_sg(torch.LongTensor(input_batch), torch.LongTensor(label_batch), all_vocabs_tensor)\n",
    "    optimizer_sg.zero_grad(); loss_sg.backward(); optimizer_sg.step()\n",
    "time_sg = time.time() - start_sg\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Word2Vec (Negative Sampling)\n",
    "# =========================================================================\n",
    "z = 0.001\n",
    "word_count = Counter(flatten(corpus))\n",
    "num_total_words = sum(word_count.values())\n",
    "unigram_table = []\n",
    "for v in vocabs:\n",
    "    uw = word_count[v] / num_total_words if v in word_count else 1/num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / z)\n",
    "    unigram_table.extend([v] * uw_alpha)\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        target_index = targets[i].item()\n",
    "        nsample = []\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index: continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "    return torch.cat(neg_samples)\n",
    "\n",
    "class SkipgramNeg(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        center_embed   = self.embedding_center(center) \n",
    "        outside_embed  = self.embedding_outside(outside) \n",
    "        negative_embed = self.embedding_outside(negative) \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) \n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(torch.sum(ukvc, 1).reshape(-1, 1))\n",
    "        return -torch.mean(loss)\n",
    "\n",
    "model_neg = SkipgramNeg(voc_size, emb_size)\n",
    "optimizer_neg = optim.Adam(model_neg.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training Skipgram (Negative Sampling)...\")\n",
    "start_neg = time.time()\n",
    "for epoch in range(1000):\n",
    "    input_batch, label_batch = random_batch(batch_size, skipgrams)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k=5)\n",
    "    loss_neg = model_neg(torch.LongTensor(input_batch), label_tensor, neg_samples)\n",
    "    optimizer_neg.zero_grad(); loss_neg.backward(); optimizer_neg.step()\n",
    "time_neg = time.time() - start_neg\n",
    "\n",
    "# =========================================================================\n",
    "# 3. GloVe\n",
    "# =========================================================================\n",
    "def build_cooccurrence(corpus, window_size):\n",
    "    cooc = Counter()\n",
    "    for doc in corpus:\n",
    "        for i in range(len(doc)):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(doc), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i == j: continue\n",
    "                cooc[(doc[i], doc[j])] += 1\n",
    "    return cooc\n",
    "\n",
    "def glove_random_batch(batch_size, cooc_counts):\n",
    "    pairs = list(cooc_counts.keys())\n",
    "    indices = np.random.choice(len(pairs), batch_size)\n",
    "    inputs, labels, coocs, weights = [], [], [], []\n",
    "    for idx in indices:\n",
    "        pair = pairs[idx]\n",
    "        count = cooc_counts[pair]\n",
    "        inputs.append([word2index[pair[0]]]); labels.append([word2index[pair[1]]])\n",
    "        coocs.append([math.log(count)])\n",
    "        weights.append([(count/100)**0.75 if count < 100 else 1.0])\n",
    "    return np.array(inputs), np.array(labels), np.array(coocs), np.array(weights)\n",
    "\n",
    "class Glove(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.center_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        self.center_bias = nn.Embedding(voc_size, 1)\n",
    "        self.outside_bias = nn.Embedding(voc_size, 1)\n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        c_e = self.center_embedding(center); o_e = self.outside_embedding(outside)\n",
    "        c_b = self.center_bias(center).squeeze(1); o_b = self.outside_bias(outside).squeeze(1)\n",
    "        inner = o_e.bmm(c_e.transpose(1, 2)).squeeze(2)\n",
    "        loss = weighting * torch.pow(inner + c_b + o_b - coocs, 2)\n",
    "        return torch.sum(loss)\n",
    "\n",
    "cooc_counts = build_cooccurrence(corpus, window_size)\n",
    "model_glove = Glove(voc_size, emb_size)\n",
    "optimizer_glove = optim.Adam(model_glove.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training GloVe...\")\n",
    "start_glove = time.time()\n",
    "for epoch in range(1000):\n",
    "    i_b, t_b, c_b, w_b = glove_random_batch(batch_size, cooc_counts)\n",
    "    loss_gv = model_glove(torch.LongTensor(i_b), torch.LongTensor(t_b), torch.FloatTensor(c_b), torch.FloatTensor(w_b))\n",
    "    optimizer_glove.zero_grad(); loss_gv.backward(); optimizer_glove.step()\n",
    "time_glove = time.time() - start_glove\n",
    "\n",
    "# =========================================================================\n",
    "# TASK 2: EVALUATION\n",
    "# =========================================================================\n",
    "# 1. Loading Analogy Data\n",
    "url = \"https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt\"\n",
    "urllib.request.urlretrieve(url, \"analogy.txt\")\n",
    "\n",
    "semantic_tests, syntactic_tests = [], []\n",
    "curr_cat = None\n",
    "with open(\"analogy.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith(':'):\n",
    "            curr_cat = line.strip()\n",
    "            continue\n",
    "        words = line.lower().split()\n",
    "        if curr_cat == ': capital-common-countries' and all(w in word2index for w in words):\n",
    "            semantic_tests.append(words)\n",
    "        elif curr_cat == ': past-tense' and all(w in word2index for w in words):\n",
    "            syntactic_tests.append(words)\n",
    "\n",
    "# 2. Solver Function\n",
    "def solver(model, a, b, c, mode='sg'):\n",
    "    def get_v(w):\n",
    "        idx = torch.LongTensor([word2index[w]])\n",
    "        if mode == 'glove': return (model.center_embedding(idx) + model.outside_embedding(idx)).detach().squeeze().numpy()\n",
    "        return (model.embedding_center(idx) + model.embedding_outside(idx)).detach().squeeze().numpy()\n",
    "    \n",
    "    target = get_v(b) - get_v(a) + get_v(c)\n",
    "    best_w, max_sim = None, -1\n",
    "    for w in vocabs:\n",
    "        if w in [a, b, c, '<UNK>']: continue\n",
    "        sim = 1 - cosine(target, get_v(w))\n",
    "        if sim > max_sim: max_sim = sim; best_w = w\n",
    "    return best_w\n",
    "\n",
    "def get_acc(model, tests, mode):\n",
    "    if not tests: return 0.0\n",
    "    correct = sum(1 for t in tests if solver(model, t[0], t[1], t[2], mode) == t[3])\n",
    "    return (correct / len(tests)) * 100\n",
    "\n",
    "# 3. Gensim Benchmark\n",
    "print(\"Loading Gensim GloVe...\")\n",
    "g_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "def get_gensim_acc(tests):\n",
    "    if not tests: return 0.0\n",
    "    c = 0\n",
    "    for t in tests:\n",
    "        try:\n",
    "            if g_model.most_similar(positive=[t[1], t[2]], negative=[t[0]], topn=1)[0][0] == t[3]: c += 1\n",
    "        except: continue\n",
    "    return (c / len(tests)) * 100\n",
    "\n",
    "# =========================================================================\n",
    "# FINAL RESULTS TABLE\n",
    "# =========================================================================\n",
    "data = [\n",
    "    [\"Skipgram\", window_size, f\"{loss_sg.item():.4f}\", f\"{time_sg:.2f}s\", get_acc(model_sg, syntactic_tests, 'sg'), get_acc(model_sg, semantic_tests, 'sg')],\n",
    "    [\"Skipgram (NEG)\", window_size, f\"{loss_neg.item():.4f}\", f\"{time_neg:.2f}s\", get_acc(model_neg, syntactic_tests, 'neg'), get_acc(model_neg, semantic_tests, 'neg')],\n",
    "    [\"Glove\", window_size, f\"{loss_gv.item():.4f}\", f\"{time_glove:.2f}s\", get_acc(model_glove, syntactic_tests, 'glove'), get_acc(model_glove, semantic_tests, 'glove')],\n",
    "    [\"Glove (Gensim)\", \"N/A\", \"N/A\", \"N/A\", get_gensim_acc(syntactic_tests), get_gensim_acc(semantic_tests)]\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*85)\n",
    "print(f\"{'Model':<18} {'Win':<5} {'Loss':<10} {'Time':<10} {'Syntactic %':<15} {'Semantic %'}\")\n",
    "print(\"-\" * 85)\n",
    "for r in data:\n",
    "    print(f\"{r[0]:<18} {r[1]:<5} {r[2]:<10} {r[3]:<10} {r[4]:<15.2f} {r[5]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f34253c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 354 word pairs.\n",
      "Standard Model Comparison Table:\n",
      "            Model  Spearman    MSE  Skipped\n",
      "0        Skipgram    -0.081  0.920      267\n",
      "1  Skipgram (NEG)     0.125  0.789      267\n",
      "2           GloVe    -0.049  0.665      267\n",
      "3  GloVe (Gensim)     0.536  0.053        0\n",
      "\n",
      "Table 1. Swapped Columns and Rows Table\n",
      "Model     Skipgram  Skipgram (NEG)    GloVe  GloVe (Gensim)\n",
      "Spearman    -0.081           0.125   -0.049           0.536\n",
      "MSE          0.920           0.789    0.665           0.053\n",
      "Skipped    267.000         267.000  267.000           0.000\n",
      "\n",
      "Assessment:\n",
      "The GloVe model Spearman correlation is -0.049. It correlates weakly with human judgment due to data limitations.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# =========================================================================\n",
    "# 1. PREPARE LOOKUP DICTIONARIES (Extracting from Trained Models)\n",
    "# =========================================================================\n",
    "\n",
    "def create_lookup(model, model_type='sg'):\n",
    "    # Get center and outside embeddings\n",
    "    if model_type == 'glove':\n",
    "        v = model.center_embedding.weight.detach()\n",
    "        u = model.outside_embedding.weight.detach()\n",
    "    else:\n",
    "        v = model.embedding_center.weight.detach()\n",
    "        u = model.embedding_outside.weight.detach()\n",
    "    \n",
    "    # Combined embedding (average)\n",
    "    W = (v + u) / 2\n",
    "    # Normalize for cosine similarity via dot product: Wn = W / ||W||\n",
    "    norm = W.norm(p=2, dim=1, keepdim=True)\n",
    "    Wn = W / norm\n",
    "    \n",
    "    return {\"stoi\": word2index, \"Wn\": Wn}\n",
    "\n",
    "# Create lookups for our trained models\n",
    "skipgram_lookup = create_lookup(model_sg, 'sg')\n",
    "skipgram_neg_lookup = create_lookup(model_neg, 'sg')\n",
    "glove_lookup = create_lookup(model_glove, 'glove')\n",
    "\n",
    "# =========================================================================\n",
    "# 2. LOAD WORDSIM353 DATASET (Corrected with Error Handling)\n",
    "# =========================================================================\n",
    "ws_path = datapath(\"wordsim353.tsv\")\n",
    "with open(ws_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    lines = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "rows = []\n",
    "for ln in lines:\n",
    "    parts = ln.split()\n",
    "    # Check if we have at least 3 parts (Word1, Word2, Score)\n",
    "    if len(parts) < 3: \n",
    "        continue\n",
    "    \n",
    "    # Use a try-except block to skip metadata/header lines\n",
    "    try:\n",
    "        w1 = parts[0].lower()\n",
    "        w2 = parts[1].lower()\n",
    "        score = float(parts[2]) # This will fail on text like 'WordSimilarity-353'\n",
    "        rows.append((w1, w2, score))\n",
    "    except ValueError:\n",
    "        # This skips the line if parts[2] is not a number\n",
    "        continue\n",
    "\n",
    "ws = pd.DataFrame(rows, columns=[\"Word 1\", \"Word 2\", \"Human (mean)\"])\n",
    "print(f\"Successfully loaded {len(ws)} word pairs.\")\n",
    "\n",
    "# =========================================================================\n",
    "# 3. SIMILARITY SCORE FUNCTIONS\n",
    "# =========================================================================\n",
    "def similarity_scores_torch(lookup, ws_df):\n",
    "    stoi_local = lookup[\"stoi\"]\n",
    "    Wn = lookup[\"Wn\"]\n",
    "    sims, gold, skipped = [], [], 0\n",
    "\n",
    "    for _, row in ws_df.iterrows():\n",
    "        w1, w2, score = row[\"Word 1\"], row[\"Word 2\"], row[\"Human (mean)\"]\n",
    "        if w1 not in stoi_local or w2 not in stoi_local:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        v1, v2 = Wn[stoi_local[w1]], Wn[stoi_local[w2]]\n",
    "        # Dot product of normalized vectors = Cosine Similarity\n",
    "        sims.append(torch.dot(v1, v2).item())\n",
    "        gold.append(score)\n",
    "    return np.array(sims), np.array(gold), skipped\n",
    "\n",
    "def similarity_scores_gensim(model, ws_df):\n",
    "    sims, gold, skipped = [], [], 0\n",
    "    for _, row in ws_df.iterrows():\n",
    "        w1, w2, score = row[\"Word 1\"], row[\"Word 2\"], row[\"Human (mean)\"]\n",
    "        if w1 not in model or w2 not in model:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        sims.append(model.similarity(w1, w2))\n",
    "        gold.append(score)\n",
    "    return np.array(sims), np.array(gold), skipped\n",
    "\n",
    "# =========================================================================\n",
    "# 4. CALCULATE METRICS\n",
    "# =========================================================================\n",
    "results_similarity = []\n",
    "\n",
    "# Eval loop for custom models\n",
    "for name, lookup in [(\"Skipgram\", skipgram_lookup), (\"Skipgram (NEG)\", skipgram_neg_lookup), (\"GloVe\", glove_lookup)]:\n",
    "    sims, gold, skipped = similarity_scores_torch(lookup, ws)\n",
    "    rho, _ = spearmanr(sims, gold)\n",
    "    # Human scores are 0-10, sims are -1 to 1. To calculate MSE, we normalize human scores to 0-1\n",
    "    mse = np.mean(((sims) - (gold/10)) ** 2) \n",
    "    results_similarity.append({\"Model\": name, \"Spearman\": rho, \"MSE\": mse, \"Skipped\": skipped})\n",
    "\n",
    "# Eval for Gensim\n",
    "sims, gold, skipped = similarity_scores_gensim(g_model, ws)\n",
    "rho, _ = spearmanr(sims, gold)\n",
    "mse = np.mean(((sims) - (gold/10)) ** 2)\n",
    "results_similarity.append({\"Model\": \"GloVe (Gensim)\", \"Spearman\": rho, \"MSE\": mse, \"Skipped\": skipped})\n",
    "\n",
    "sim_df = pd.DataFrame(results_similarity)\n",
    "\n",
    "# =========================================================================\n",
    "# 5. FINAL MERGED TABLE & TABLE 1 (SWAPPED)\n",
    "# =========================================================================\n",
    "# (Assuming analogy_df and training_df were created in Task 2 parts 1 & 2)\n",
    "# Here we create a dummy summary for the final merge\n",
    "final_table = sim_df.copy()\n",
    "\n",
    "# ROUNDING\n",
    "final_table[\"Spearman\"] = final_table[\"Spearman\"].round(3)\n",
    "final_table[\"MSE\"] = final_table[\"MSE\"].round(3)\n",
    "\n",
    "print(\"Standard Model Comparison Table:\")\n",
    "print(final_table)\n",
    "\n",
    "# TABLE 1: Swapped Columns and Rows Table\n",
    "table_1_swapped = final_table.set_index(\"Model\").T\n",
    "\n",
    "print(\"\\nTable 1. Swapped Columns and Rows Table\")\n",
    "print(table_1_swapped)\n",
    "\n",
    "# Assessment\n",
    "print(\"\\nAssessment:\")\n",
    "rho_val = final_table.loc[final_table['Model'] == 'GloVe', 'Spearman'].values[0]\n",
    "if rho_val < 0.2:\n",
    "    print(f\"The GloVe model Spearman correlation is {rho_val}. It correlates weakly with human judgment due to data limitations.\")\n",
    "else:\n",
    "    print(f\"The GloVe model Spearman correlation is {rho_val}, showing some alignment with human judgment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcb8c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Function to convert any text into a vector (Average of word embeddings)\n",
    "def get_text_vector(text, model, word2index):\n",
    "    tokens = text.lower().split()\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in word2index:\n",
    "            idx = torch.LongTensor([word2index[token]])\n",
    "            # Using GloVe average of center and outside\n",
    "            embed = (model.center_embedding(idx) + model.outside_embedding(idx)) / 2\n",
    "            vectors.append(embed.detach().squeeze().numpy())\n",
    "    \n",
    "    if not vectors: # If no words in vocab, return zero vector\n",
    "        return np.zeros(emb_size)\n",
    "    \n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# 2. Pre-calculate vectors for the entire corpus (Top 500 docs for speed)\n",
    "reuters_docs = df['text'].astype(str).tolist()[:500]\n",
    "corpus_vectors = np.array([get_text_vector(doc, model_glove, word2index) for doc in reuters_docs])\n",
    "\n",
    "# 3. Search function\n",
    "def search(query, top_n=10):\n",
    "    query_vec = get_text_vector(query, model_glove, word2index)\n",
    "    # Compute dot product against all documents\n",
    "    scores = np.dot(corpus_vectors, query_vec)\n",
    "    # Get indices of top 10 scores\n",
    "    top_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            \"text\": reuters_docs[idx][:200] + \"...\", # Show snippet\n",
    "            \"score\": round(float(scores[idx]), 4)\n",
    "        })\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
